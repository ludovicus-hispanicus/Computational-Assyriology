{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.2 Parsing ETCSL\n",
    "## 2.2.1. Introduction\n",
    "\n",
    "The Electronic Text Corpus of Sumerian Literature ([ETCSL](http://etcsl.orinst.ox.ac.uk)) provides editions and translations of almost 400 Sumerian literary texts, mostly from the Old Babylonian period (around 1800 BCE). The project was founded by Jeremy Black (Oxford University), who sadly passed away in 2004; it was active from 1998 to 2006, when it was archived. Information about the project, its stages, products and collaborators may be found in the project's [About](http://etcsl.orinst.ox.ac.uk/edition2/general.php) page. By the time of its inception [ETCSL](http://etcsl.orinst.ox.ac.uk) was a pioneering effort - the first large digital project in Assyriology, using well-structured data according to the standards and best practices of the time. [ETCSL](http://etcsl.orinst.ox.ac.uk) allows for various kinds of searches in Sumerian and in English translation and provides lemmatization for each individual word. Numerous scholars contributed data sets to the [ETCSL](http://etcsl.orinst.ox.ac.uk) project (see [Acknowledgements](http://etcsl.orinst.ox.ac.uk/edition2/credits.php#ack)). The availability of [ETCSL](http://etcsl.orinst.ox.ac.uk) has fundamentally altered the study of Sumerian literature and has made this literature available for undergraduate teaching.\n",
    "\n",
    "The original [ETCSL](http://etcsl.orinst.ox.ac.uk) files in TEI XML are stored in the [Oxford Text Archive](http://hdl.handle.net/20.500.12024/2518) from where they can be downloaded as a ZIP file under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License ([by-nc-sa 3.0](http://creativecommons.org/licenses/by-nc-sa/3.0/)). The copyright holders are Jeremy Black, Graham Cunningham, Jarle Ebeling, Esther Flückiger-Hawker, Eleanor Robson, Jon Taylor, and Gábor Zólyomi.\n",
    "\n",
    "The [Oxford Text Archive](http://hdl.handle.net/20.500.12024/2518) page offers the following description:\n",
    "\n",
    "> The Electronic Text Corpus of Sumerian Literature (ETCSL) comprises transliterations and English translations of 394 compositions attested on sources dating to the period from approximately 2100 to 1700 BCE. The compositions are divided into seven categories: ancient literary catalogues; narrative compositions; royal praise poetry and hymns to deities on behalf of rulers; literary letters and letter-prayers; divine and temple hymns; proverbs and proverb collections; and a more general category including compositions such as debates, dialogues and riddles. The numbering of the compositions within the corpus follows Miguel Civil's unpublished catalogue of Sumerian literature (etcslfullcat.html).Files with an initial c are composite transliterations (a reconstructed text editorially assembled from the extant exemplars but including substantive variants) in which the cuneiform signs are represented in the Roman alphabet. Files with an initial t are translations. The composite files include full references for the cuneiform sources and author-date references for the secondary sources (detailed in bibliography.xml). The composite and translation files are in XML and have been annotated according to the TEI guidelines. In terms of linguistic information, each word form in the composite transliterations has been assigned to a lexeme which is specified by a citation form, word class information and basic English translation.\n",
    "\n",
    "Since [ETCSL](http://etcsl.orinst.ox.ac.uk) is an archival site, the editions are not updated to reflect new text finds or new insights in the Sumerian language. Many of the [ETCSL](http://etcsl.orinst.ox.ac.uk) editions were based on standard print editions that itself may have been 10 or 20 years old by the time they were digitized. Any computational analysis of the [ETCSL](http://etcsl.orinst.ox.ac.uk) corpus will have to deal with the fact that: \n",
    "\n",
    "- the text may not represent the latest standard\n",
    "- the [ETCSL](http://etcsl.orinst.ox.ac.uk) corpus is extensive - but does not cover all of Sumerian literature known today\n",
    "\n",
    "In terms of data acquisition, one way to deal with these limitations is to make the [ETCSL](http://etcsl.orinst.ox.ac.uk) data as much as possible compatible with the data standards of the Open Richly Annotated Cuneiform Corpus ([ORACC](http://oracc.org)). [ORACC](http://oracc.org) is an active project where new or updated editions can be produced. If compatible, if [ETCSL](http://etcsl.orinst.ox.ac.uk) and [ORACC](http://oracc.org) data may be freely mixed and matched, then the [ETCSL](http://etcsl.orinst.ox.ac.uk) data set can effectively be updated and expanded.\n",
    "\n",
    "The [ETCSL](http://etcsl.orinst.ox.ac.uk) text corpus was one of the core data sets for the development of of [ePSD1](http://psd.museum.upenn.edu/epsd1/index.html) and [ePSD2][], and the ePSD version of the [ETCSL](http://etcsl.orinst.ox.ac.uk) data forms the core of the literary corpus collected in [ePSD2/literary](http://oracc.org/epsd2/literary). In order to harvest the [ETCSL](http://etcsl.orinst.ox.ac.uk) data for [ePSD2][] the lemmatization was adapted to [ORACC](http://oracc.org) standards and thus the [ePSD2/literary](http://oracc.org/epsd2/literary) version of the [ETCSL](http://etcsl.orinst.ox.ac.uk) dataset is fully compatible with any [ORACC](http://oracc.org) dataset, and can be parsed with the ORACC parser, discussed in section 2.1. However, [ePSD2/literary](http://oracc.org/epsd2/literary) is not identical with the [ETCSL](http://etcsl.orinst.ox.ac.uk) dataset. Several compositions have been replaced by more recent editions (for instance the Sumerian disputations edited in the [ORACC](http://oracc.org) project [Datenbank Sumerischer Streitliteratur](http://oracc.org/dsst)); a significant number of texts that were not available in [ETCSL](http://etcsl.orinst.ox.ac.uk) have been added (many of them published after 2006) and the Gudea Cylinders have been moved to the [epsd2/royal](http://oracc.org/epsd2/royal).\n",
    "\n",
    "For some applications, therefore, parsing the original [ETCSL](http://etcsl.orinst.ox.ac.uk) XML TEI files has become redundant. However, any data transformation implies choices and it is hard to know what the needs will be of future computational approaches to the [ETCSL](http://etcsl.orinst.ox.ac.uk) dataset. The reason to include and discuss the [ETCSL](http://etcsl.orinst.ox.ac.uk) parser here is, first, to offer users the opportunity to work with the original data set. The various transformations included in the current parser may be adapted and adjusted to reflect the preferences and research questions of the user. As a concrete example of choices to be made, [ETCSL](http://etcsl.orinst.ox.ac.uk) distinguishes between main text, secondary text, and additional text, to reflect different types of variants between manuscripts (see below [2.2.4](#2.2.4-Pre-Processing:-Additional-Text-and-Secondary-Text)). The [ePSD2/literary](http://oracc.org/epsd2/literary) data set does not include this distinction. The output of the current parser will indicate for each word whether it is \"secondary\" or \"additional\" (according to [ETCSL](http://etcsl.orinst.ox.ac.uk) criteria) and offer the possibility to include such words or exclude them from the analysis. Similarly, the translations are not included in the [ePSD2/literary](http://oracc.org/epsd2/literary) dataset, nor are they considered by the present parser. Translation data are, however, available in the [ETCSL](http://etcsl.orinst.ox.ac.uk) XML TEI file set and the XML of the transcription files marks the beginning and end of translation paragraphs. Such data, therefore, is available and one may well imagine research questions for which the translation files are relevant (e.g. translation alignment). Although the present code does not deal with translation, one may use the same techniques and the same approach exemplified here to retrieve such data.\n",
    "\n",
    "In order to achieve compatibility between [ETCSL](http://etcsl.orinst.ox.ac.uk) and [ORACC](http://oracc.org) the code uses a number of equivalence dictionaries, that enable replacement of characters, words, or names. These equivalence dictionaries are stored in JSON format (for JSON see section 2.1.1) in the file `equivalencies.json`  in the directory `equivalencies`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1.1 XML\n",
    "The [ETCSL](http://etcsl.orinst.ox.ac.uk) files as distributed by the [Oxford Text Archive](http://hdl.handle.net/20.500.12024/2518) are encoded in a dialect of `XML` (Extensible Markup Language) that is referred to as `TEI` (Text Encoding Initiative). In this encoding each word (in transliteration) is an *element* that is surrounded by `<w>` and `</w>` tags. Inside the start-tag the word may receive several attributes, encoded as name/value pairs, as in the following random examples:\n",
    "\n",
    "```xml\n",
    "<w form=\"ti-a\" lemma=\"te\" pos=\"V\" label=\"to approach\">ti-a</w>\n",
    "<w form=\"e2-jar8-bi\" lemma=\"e2-jar8\" pos=\"N\" label=\"wall\">e2-jar8-bi</w>\n",
    "<w form=\"ickila-bi\" lemma=\"ickila\" pos=\"N\" label=\"shell\"><term id=\"c1813.t1\">ickila</term><gloss lang=\"sux\" target=\"c1813.t1\">la</gloss>-bi</w>\n",
    "```\n",
    "\n",
    "The `form` attribute is the full form of the word, including morphology, but omitting flags (such as question marks), indication of breakage, or glosses. The `lemma` attribute is the form minus morphology (similar to `Citation Form` in [ORACC](http://oracc.org). Some lemmas may be spelled in more than one way in Sumerian; the `lemma` attribute will use a standard spelling (note, for instance, that the `lemma` of \"ti-a\" is \"te\"). The `lemma` in [ETCSL](http://etcsl.orinst.ox.ac.uk) (unlike `Citation Form` in [ORACC](http://oracc.org)) uses actual transliteration with hyphens and sign index numbers (as in `lemma = \"e2-jar8\"`, where the corresponding [ORACC](http://oracc.org) `Citation Form` is [egar](http://oracc.org/epsd2/o0026723).\n",
    "\n",
    ":::{note}\n",
    "| Project  | transliteration  | dictionary form   | Part of Speech    | Meaning |\n",
    "| --- | --- | --- | --- | ---- |\n",
    "| ETCSL | form \"e2-jar8-bi\" | lemma \"e2-jar8\"| pos \"N\" | label \"wall\" |\n",
    "| ORACC | form \"e₂-gar₈-bi | citation form \"egar\" | pos \"N\" | guide word \"wall\"|\n",
    ":::\n",
    "\n",
    "\n",
    "The `label` attribute gives a general indication of the meaning of the Sumerian word but is not context-sensitive. That is, the `label` of \"lugal\" is always \"king\", even if in context the word means \"owner\". The `pos` attribute gives the Part of Speech, but again the attribute is not context-sensitive. Where a verb (such as sag₉, to be good) is used as an adjective the `pos` is still \"V\" (for verb). Together `lemma`, `label`, and `pos` define a Sumerian lemma (dictionary entry).\n",
    "\n",
    "In parsing the [ETCSL](http://etcsl.orinst.ox.ac.uk) files we will be looking for the `<w>` and `</w>` tags to isolate words and their attributes. Higher level tags identify lines (`<l>` and `</l>`), versions, secondary text (found only in a minority of sources), etcetera.\n",
    "\n",
    "The [ETCSL](http://etcsl.orinst.ox.ac.uk) file set includes the file [etcslmanual.html](http://etcsl.orinst.ox.ac.uk/edition2/etcslmanual.php) with explanations of the tags, their attributes, and their proper usage.\n",
    "\n",
    "Goal of the parsing process is to get as much information as possible out of the `XML` tree in a format that is as close as possible to the output of the [ORACC](http://oracc.org/) parser. The output of the parser is a word-by-word (or rather lemma-by-lemma) representation of the entire [ETCSL](http://etcsl.orinst.ox.ac.uk) corpus. For most computational projects it will be necessary to group words into lines or compositions, or to separate out a particular group of compositions. The data is structured in such a way that that can be achieved with a standard set of Python functions of the `pandas` library.\n",
    "\n",
    "### 2.2.1.2 Parsing XML: Xpath, and lxml\n",
    "\n",
    ":::{margin}\n",
    "For proper introductions to `Xpath` and `lxml` see the [Wikipedia](https://en.wikipedia.org/wiki/XPath) article on `Xpath` and the homepage of the [`lxml`](https://lxml.de/) library, respectively.\n",
    ":::\n",
    "\n",
    "There are several Python libraries specifically for parsing `XML`, among them the popular `ElementTree` and its twin `cElementTree`. The library `lxml` is largely compatible with `ElementTree` and `cElementTree` but differs from those in its full support of `Xpath`. `Xpath` is a language for finding and retrieving elements and attributes in XML trees. `Xpath` is not a program or a library, but a set of specifications that is implemented in a variety of software packages in different programming languages. \n",
    "\n",
    "`Xpath` uses the forward slash to describe a path through the hierarchy of the `XML` tree. The expression `\"/body/l/w\"` refers to all the `w` (word) elements that are children of `l` (line) elements that are children of the `body` element in the top level of `XML` hierarchy.\n",
    "\n",
    "The expression `'//w'`means: all the `w` nodes, wherever in the hierarchy of the `XML` tree. The expression may be used to create a list of all the `w` nodes with all of their associated attributes. The attributes of a node are addressed  with the `@` sign, so that `//w/@label` refers to the `label` attributes of all the `w` nodes at any level in the hierarchy. \n",
    "\n",
    "```python\n",
    "words = tree.xpath('//w')\n",
    "labels = tree.xpath('//w/@label')\n",
    "```\n",
    "\n",
    "Predicates are put between square brackets and describe conditions for filtering a node set. The expression  `//w[@emesal]` will return all the `w` elements that have an attribute `emesal`. \n",
    "\n",
    "`Xpath` also defines hundreds of functions. An important function is `'string()'` which will return the string value of a node or an attribute.  Once all `w` nodes are listed in the list `words` (with the code above) one may extract the transliteration and Guide Word (`label` in [ETCSL](http://etcsl.orinst.ox.ac.uk)) of each word as follows:\n",
    "\n",
    "```python\n",
    "form_l = []\n",
    "gw_l = []\n",
    "for node in words:\n",
    "    form = node.xpath('string(.)') \n",
    "    form_l.append(form)\n",
    "    gw = node.xpath('string(@label)')\n",
    "    gw_l.append(gw)\n",
    "```\n",
    "\n",
    "The dot, the argument to the `string()` function in `node.xpath('string(.)')`, refers to the current node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1.2 Input and Output\n",
    "\n",
    "This scraper expects the following files and directories:\n",
    "\n",
    "1. Directory `etcsl/transliterations/`  \n",
    "   This directory should contain the [ETCSL](http://etcsl.orinst.ox.ac.uk) `TEI XML` transliteration files. The files may be downloaded from the [Oxford Text Archive](http://hdl.handle.net/20.500.12024/2518). The files are found in the file `etcsl.zip`, in the directory `transliterations`.\n",
    "2. Directory `Equivalencies`  \n",
    "   `equivalencies.json`: a set of equivalence dictionaries used at various places in the parser.  \n",
    "\n",
    "The output is saved in the `output` directory as a single `.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Setting Up\n",
    "### 2.2.2.1 Load Libraries\n",
    "Before running this cell you may need to install the package `lxml` (for parsing `XML`) by running \n",
    "```python\n",
    "%conda intstall lxml\n",
    "```\n",
    ":::{margin}\n",
    "For proper installation of packages for a Jupyter Notebook see [1.4.3 install_packages](../1_Preliminaries/1_Introduction.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from lxml import etree\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "os.makedirs('output', exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2.2 Load Equivalencies \n",
    "The file `equivalencies.json` contains a number of dictionaries that will be used to search and replace at various places in this notebook. The dictionaries are:\n",
    "- `suxwords`: Sumerian words (Citation Form, GuideWord, and Part of Speech) in [ETCSL](http://etcsl.orinst.ox.ac.uk) format and their [ORACC](http://oracc.org) counterparts.\n",
    "- `emesalwords`: idem for Emesal words\n",
    "- `propernouns`: idem for proper nouns\n",
    "- `ampersands`: HTML entities (such as `&aacute;`) and their Unicode counterparts (`á`; see section 2.2.3).\n",
    "- `versions`: [ETCSL](http://etcsl.orinst.ox.ac.uk) version names and (abbreviated) equivalents\n",
    "\n",
    "The `equivalencies.json` file is loaded with the `json` library. The dictionaries `suxwords`, `emesalwords` and `propernouns` (which, together, contain the entire [ETCSL](http://etcsl.orinst.ox.ac.uk) vocabulary) are concatenated into a single dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"equivalencies/equivalencies.json\", encoding=\"utf-8\") as f:\n",
    "    eq = json.load(f)\n",
    "equiv = eq[\"suxwords\"]\n",
    "equiv.update(eq[\"emesalwords\"])\n",
    "equiv.update(eq[\"propernouns\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Preprocessing: HTML-entities\n",
    "Before the XML files can be parsed, it is necessary to remove character sequences that are not allowed in XML proper (so-called HTML entities). \n",
    "\n",
    "In non-transliteration contexts (bibliographies, composition titles, etc.) [ETCSL](https://etcsl.orinst.ox.ac.uk/) uses so-called HTML entities to represent non-ASCII characters such as,  á, ü, or š. These entities are encoded with an opening ampersand (`&`) and a closing semicolon (`;`). For instance, `&C;` represents the character `Š`. The HTML entities are for the most part project-specific and are declared in the file `etcsl-sux.ent` which is part of the file package and is used by the [ETCSL](https://etcsl.orinst.ox.ac.uk/) project in the process of validating and parsing the XML for on-line publication.\n",
    "\n",
    "For purposes of data acquisition these entities need to be resolved, because XML parsers will not recognize these sequences as valid XML. \n",
    "\n",
    "The key `ampersands` in the file `equivalencies.json` has as its value a dictionary, listing all the HTML entities that appear in the [ETCSL](https://etcsl.orinst.ox.ac.uk/) files with their Unicode counterparts:\n",
    "\n",
    "```json\n",
    "{ \n",
    " \"&C;\": \"Š\",\n",
    " \"&Ccedil;\": \"Ç\",\n",
    " \"&Eacute;\": \"É\",\n",
    " \"&G;\": \"Ŋ\",\n",
    " \"&H;\": \"H\",\n",
    " \"&Imacr;\": \"Î\",\n",
    " \"&X;\" : \"X\",\n",
    " \"&aacute;\": \"á\"\n",
    "}\n",
    "```\n",
    "\n",
    "etc.\n",
    "\n",
    "This dictionary is used to replace each HTML entity with its unicode (UTF-8) counterpart in the entire corpus. The function `ampersands()` is called in the function `parsetext()` immediately after opening the file of one of the compositions in [ETCSL](https://etcsl.orinst.ox.ac.uk/).  \n",
    "\n",
    ":::{margin}\n",
    "The expression `[^;]+` means: a sequence of one or more (`+`) characters, except the semicolon. The symbol `^` is the negation symbol in regular expressions. The expression `&[^;]+;` therefore captures a sequence of any length that begins with an ampersand and ends with a semicolon. There are many introductions for regular expressions on the web, for instance [regular-expressions.info](https://www.regular-expressions.info/), or [An Introduction to Regular Expressions](https://www.oreilly.com/content/an-introduction-to-regular-expressions/) by Thomas Nield.\n",
    ":::\n",
    "\n",
    "The function `ampersands()` uses the `sub()` function from the `re` (Regular Expressions) module. The arguments of this function are `sub(find_what, replace_with, text)`. In this case, the `find_what` is the compiled regular expression `amp`, matching all character sequences that begin with & and end with a semicolon (;). This regular expression is defined in the main process (see section 2.2.12) as follows:\n",
    "\n",
    "```python\n",
    "amp = re.compile(r'&[^;]+;')\n",
    "```\n",
    "\n",
    "The `replace_with` argument is a temporary `lambda` function that uses the `ampersands` dictionary to find the utf-8 counterpart of the HTML entity. The dictionary is queried with the `get()` function (m.group() represents the match of the regular expression `amp`). The `get()` function allows a fall-back argument, to be returned in case the dictionary does not have the key that was requested. This second argument is the actual regular expression match, so that in those cases where the dictionary does not contain the match it is replaced by itself.\n",
    "\n",
    "The [ETCSL](http://etcsl.orinst.ox.ac.uk) `TEI XML` files are written in ASCII and represent special characters (such as š or ī) by a sequence of characters that begins with & and ends with ; (e.g. `&c;` represents `š`). The `lxml` library cannot deal with these entities and thus we have to replace them with the actual (Unicode) character that they represent before feeding the data to `etree` module.\n",
    "\n",
    "The function `ampersands()` uses the dictionary `ampersands` for a search-replace action. The dictionary `ampersands` is included in the file `equivalencies.json`, which was loaded above (section 2).\n",
    "\n",
    "The function `ampersands()` is called in `parsetext()` (see section 11) before the `etree` is built. The regular expression `amp` captures all so-called HTML entities (beginning with a '&' and ending with a ';'). The regex is compiled in the main process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ampersands(string):    \n",
    "    string = re.sub(amp, lambda m: \n",
    "               eq[\"ampersands\"].get(m.group(0), m.group(0)), string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4 Marking 'Secondary Text' and/or 'Additional Text'\n",
    "\n",
    "In order to be able to preserve the [ETCSL](http://etcsl.orinst.ox.ac.uk) distinctions between main text (the default), secondary text, and additional text, such information needs to be added as an attribute to each `w` node (word node). This must take place in pre-processing, before the `XML` file is parsed.\n",
    "\n",
    "[ETCSL](http://etcsl.orinst.ox.ac.uk) transliterations represent composite texts, put together (in most cases) from multiple exemplars. The editions include substantive variants, which are marked either as \"additional\" or as \"secondary\". Additional text consists of words or lines that are *added* to the text in a minority of sources. In the opening passage of [Inana's Descent to the Netherworld](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.4.1&amp;amp;amp;display=Crit&amp;amp;amp;charenc=gcirc#), for instance, there is a list of temples that Inana leaves behind. One exemplar extends this list with eight more temples; in the composite text these lines are marked as \"additional\" and numbered lines 13A-13H. Secondary text, on the other hand, is variant text (words or lines) that are found in a minority of sources *instead of* the primary text. An example in [Inana's Descent to the Netherworld](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.4.1&amp;amp;amp;display=Crit&amp;amp;amp;charenc=gcirc#) is lines 30-31, which are replaced by 30A-31A in one manuscript (text and translation from [ETCSL](http://etcsl.orinst.ox.ac.uk)):\n",
    "\n",
    "| line | text                                       | translation                                                  |\n",
    "| ---- | ------------------------------------------ | ------------------------------------------------------------ |\n",
    "| 30   | sukkal e-ne-eĝ₃ sag₉-sag₉-ga-ĝu₁₀          | my minister who speaks fair words,                           |\n",
    "| 31   | ra-gaba e-ne-eĝ₃ ge-en-gen₆-na-ĝu₁₀        | my escort who speaks trustworthy words                       |\n",
    "| 30A  | \\[na\\] ga-e-de₅ na de₅-ĝu₁₀ /ḫe₂\\\\-\\[dab₅\\]    | I am going to give you instructions: my instructions must be followed; |\n",
    "| 31A  | \\[inim\\] ga-ra-ab-dug₄ ĝizzal \\[ḫe₂-em-ši-ak\\] | I am going to say something to you: it must be observed      |\n",
    "\n",
    "\"Secondary text\" and \"additional text\" can also consist of a single word and there are even cases of \"additional text\" within \"additional text\" (an additional word within an additional line).\n",
    "\n",
    "In [ETCSL](http://etcsl.orinst.ox.ac.uk) TEI XML secondary/additional text is introduced by a tag of the type:\n",
    "\n",
    "```xml\n",
    "<addSpan to=\"c141.v11\" type=\"secondary\"/>\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```xml\n",
    "<addSpan to=\"c141.v11\" type=\"additional\"/>\n",
    "```\n",
    "\n",
    "The number c141 represents the text number in [ETCSL](http://etcsl.orinst.ox.ac.uk) (in this case [Inana's Descent to the Netherworld](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.4.1&amp;amp;amp;display=Crit&amp;amp;amp;charenc=gcirc#), text c.1.4.1). The return to the primary text is indicated by a tag of the type:\n",
    "\n",
    "```xml\n",
    "<anchor id=\"c141.v11\"/>\n",
    "```\n",
    "\n",
    "Note that the `id` attribute in the `anchor` tag is identical to the `to` attribute in the `addSpan` tag.\n",
    "\n",
    "We can collect all the `w` tags (words) between `addSpan` and its corresponding `anchor` tag with the following `xpath` expression:\n",
    "\n",
    "```python\n",
    "secondary = tree.xpath('//w[preceding::addSpan[@type=\"secondary\"]/@to = following::anchor/@id]')\n",
    "```\n",
    "\n",
    "In the expression `preceding` and `following` are so-called `axes` (plural of `axis`) which describe the relationship of an element to another element in the tree. The expression means: get all `w` tags that are preceded by an `addSpan` tag and followed by an `anchor` tag. The `addSpan` tag has to have an attribute `type` with value `secondary` , and the value of the `to` attribute of this `addSpan` tag is to be equal to the `id` attribute of the following `anchor` tag.\n",
    "\n",
    "Once we have collected all the \"secondary\" `w` tags this way, we can add a new attribute to each of these words in the following way:\n",
    "\n",
    "```python\n",
    "for word in secondary:\n",
    "    word.attrib[\"status\"] = \"secondary\"\n",
    "```\n",
    "\n",
    "In the process of parsing we can retrieve this new `status` attribute to mark all of these words as `secondary`.\n",
    "\n",
    "Since we can do exactly the same for \"additional text\" we can slightly adapt the above expression for use in the function `mark_extra()`\n",
    "\n",
    "The function `mark_extra()` is called twice by the function `parsetext()` (see below, section 2.2.11), once for \"additional\" and once for \"secondary\" text, indicated by the `which` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_extra(tree, which):\n",
    "    extra = tree.xpath(f'//w[preceding::addSpan[@type=\"{which}\"]/@to = following::anchor/@id]')\n",
    "    for word in extra:\n",
    "        word.attrib[\"status\"] = which\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.5 Transliteration Conventions\n",
    "\n",
    "Transliteration of Sumerian text in [ETCSL](http://etcsl.orinst.ox.ac.uk) `TEI XML` files uses **c** for **š**, **j** for **ŋ** and regular numbers for index numbers. The function `tounicode()` replaces each of those. For example **cag4** is replaced by **šag₄**. This function is called in the function `getword()` to format `Citation Forms` and `Forms` (transliteration). The function `tounicode()` uses the translation tables `transind` (for index numbers) and `transcj` (for c and j), defined in the main process. The `translate()` function replaces individual characters from a string, according to the table.\n",
    "\n",
    "In order to replace regular numbers with index numbers the function uses a [regular expression](https://www.regular-expressions.info/) to select only those single or double digit numbers that are preceded by a letter (leaving alone the \"7\" in 7-ta-am3). The regex `ind` is compiled in the main process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tounicode(string):\n",
    "    string = re.sub(ind, lambda m: m.group().translate(transind), string)\n",
    "    string = string.translate(transcj)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.6 Replace [ETCSL](http://etcsl.orinst.ox.ac.uk) by [ORACC](http://oracc.org) Lemmatization\n",
    "For every word, once `cf` (Citation Form), `gw` (Guide Word), and `pos` (Part of Speech) have been pulled out of the [ETCSL](http://etcsl.orinst.ox.ac.uk) `XML` file, they are combined into a lemma and run through the etcsl/oracc equivalence lists to match it with the [ORACC](http://oracc.org)/[ePSD2](http://oracc.org/epsd2) standards. The equivalence lists are stored in the file `equivalencies.json`, which was loaded above (section 2).\n",
    "\n",
    "The function `etcsl_to_oracc()` is called by the function `getword()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etcsl_to_oracc(word):\n",
    "    lemma = f\"{word['cf']}[{word['gw']}]{word['pos']}\"\n",
    "    if lemma in equiv:\n",
    "        word['cf'] = equiv[lemma][\"cf\"]\n",
    "        word[\"gw\"] = equiv[lemma][\"gw\"]\n",
    "        word[\"pos\"] = equiv[lemma][\"pos\"]\n",
    "        alltexts.append(word)\n",
    "        if \"cf2\" in equiv[lemma]: # if an ETCSL word is replaced by two ORACC words\n",
    "            word2 = word.copy()\n",
    "            word2[\"cf\"] = equiv[lemma][\"cf2\"]\n",
    "            word2[\"gw\"] = equiv[lemma][\"gw2\"]\n",
    "            word2[\"pos\"] = equiv[lemma][\"pos2\"]\n",
    "            alltexts.append(word2)\n",
    "    else: # word not found in equiv\n",
    "        alltexts.append(word)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.7 Formatting Words\n",
    "\n",
    "A word in the [ETCSL](http://etcsl.orinst.ox.ac.uk) files is represented by a `<w>` node in the `XML` tree with a number of attributes that identify the `form` (transliteration), `citation form`, `guide word`, `part of speech`, etc. The function `getword()` formats the word as closely as possible to the [ORACC](http://oracc.org) conventions. Three different types of words are treated in three different ways: Proper Nouns, Sumerian words and Emesal words.\n",
    "\n",
    "In [ETCSL](http://etcsl.orinst.ox.ac.uk) **proper nouns** are nouns (`pos` = \"N\"), which are qualified by an additional attribute `type` (Divine Name, Personal Name, Geographical Name, etc.; abbreviated as DN, PN, GN, etc.). In [ORACC](http://oracc.org) a word has a single `pos`; for proper nouns this is DN, PN, GN, etc. - so what is `type` in [ETCSL](http://etcsl.orinst.ox.ac.uk) becomes `pos` in [ORACC](http://oracc.org). [ORACC](http://oracc.org) proper nouns usually do not have a guide word (only a number to enable disambiguation of namesakes). The [ETCSL](http://etcsl.orinst.ox.ac.uk) guide words (`label`) for names come pretty close to [ORACC](http://oracc.org) citation forms. Proper nouns are therefore formatted differently from other nouns.\n",
    "\n",
    "**Sumerian words** are essentially treated in the same way in [ETCSL](http://etcsl.orinst.ox.ac.uk) and [ORACC](http://oracc.org), but the `citation forms` and `guide words` are often different. Transformation of citation forms and guide words to [ORACC](http://oracc.org)/[epsd2](http://oracc.org/epsd2/sux) standards takes place in the function `etcsl_to_oracc()` (see above, section 6).\n",
    "\n",
    "**Emesal words** in [ETCSL](http://etcsl.orinst.ox.ac.uk) use their Sumerian equivalents as `citation form` (attribute `lemma`), adding a separate attribute (`emesal`) for the Emesal form proper. This Emesal form is the one that is used as `citation form` in the output.\n",
    "\n",
    "The function `getword()` uses the dictionary `meta_d` which has collected all the meta-data (text ID, composition name, version, line number, etc.) of this particular word It produces the dictionary `word` which is sent to the function `etcsl_to_oracc()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getword(node):\n",
    "    word = {key:meta_d[key] for key in meta_d} # copy all meta data from metad_d into the word dictionary\n",
    "    if node.tag == 'gloss': # these are Akkadian glosses which are not lemmatized\n",
    "        form = node.xpath('string(.)')\n",
    "        form = form.replace('\\n', ' ').strip() # occasionally an Akkadian gloss may consist of multiple lines\n",
    "        word[\"form\"] = tounicode(form) # check - is this needed?\n",
    "        word[\"lang\"] = node.xpath(\"string(@lang)\")\n",
    "        alltexts.append(word)\n",
    "        return\n",
    "    \n",
    "    word[\"cf\"] = node.xpath('string(@lemma)') # xpath('@lemma') returns a list. The string\n",
    "    word[\"cf\"] = word[\"cf\"].replace('Xbr', '(X)')  # function turns it into a single string\n",
    "    word[\"gw\"] = node.xpath('string(@label)')\n",
    "\n",
    "    if len(node.xpath('@pos')) > 0:\n",
    "        word[\"pos\"] = node.xpath('string(@pos)')\n",
    "    else:         # if a word is not lemmatized (because it is broken or unknown) add pos = NA and gw = NA\n",
    "        word[\"pos\"] = 'NA'\n",
    "        word[\"gw\"] = 'NA'\n",
    "\n",
    "    form = node.xpath('string(@form)')\n",
    "    word[\"form\"] = form.replace('Xbr', '(X)')\n",
    "    \n",
    "    if len(node.xpath('@emesal')) > 0:\n",
    "        word[\"cf\"] = node.xpath('string(@emesal)')\n",
    "        word[\"lang\"] = \"sux-x-emesal\"\n",
    "    else:\n",
    "        word[\"lang\"] = \"sux\"\n",
    "\n",
    "    exception = [\"unclear\", \"Mountain-of-cedar-felling\", \"Six-headed Wild Ram\", \n",
    "                     \"The-enemy-cannot-escape\", \"Field constellation\", \n",
    "                     \"White Substance\", \"Chariot constellation\", \n",
    "                 \"Crushes-a-myriad\", \"Copper\"]\n",
    "    \n",
    "    if len(node.xpath('@type')) > 0 and word[\"pos\"] == 'N': # special case: Proper Nouns\n",
    "        if node.xpath('string(@type)') != 'ideophone':  # special case in the special case: skip ideophones\n",
    "            word[\"pos\"] = node.xpath('string(@type)')\n",
    "            word[\"gw\"] = '1'\n",
    "            if node.xpath('string(@label)') not in exception:\n",
    "                word[\"cf\"] = node.xpath('string(@label)')\n",
    "    if len(node.xpath('@status')) > 0:\n",
    "        word['status'] = node.xpath('string(@status)')\n",
    "    \n",
    "    word[\"cf\"] = tounicode(word[\"cf\"])\n",
    "    word[\"form\"] = tounicode(word[\"form\"])\n",
    "    etcsl_to_oracc(word)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.8 Formatting Lines\n",
    "\n",
    "A line may either be an actual line (in Sumerian and/or Akkadian) or a gap (a portion of text lost). Both receive a line reference. A line reference is an integer that is used to keep lines (and gaps) in their proper order.\n",
    "\n",
    "A gap of one or more lines in the composite text, due to damage to the original cuneiform tablet, is encoded as follows:\n",
    "\n",
    "```xml\n",
    "<gap extent=\"8 lines missing\"/>\n",
    "```\n",
    "\n",
    "In order to be able to process this information and keep it at the right place in the data we will parse the `gap` tags together with the `l` (line) tags and process the gap as a line. In [ORACC](http://oracc.org) gaps are described with the fields `extent` (a number, or `n` for unknown),  and `scope` (line, column, obverse, etc.) . [ORACC](http://oracc.org) uses a restricted vocabulary for these fields, but [ETCSL](https://etcsl.orinst.ox.ac.uk/) does not. The code currently does not try to make the [ETCSL](https://etcsl.orinst.ox.ac.uk/) encoding of gaps compatible with the [ORACC](http://oracc.org) encoding.\n",
    "\n",
    "The function `getline()` is called by `getsection()`. If the argument of `getline()` is an actual line (not a gap) it calls `getword()` for every individual word in that line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getline(lnode):\n",
    "    meta_d[\"id_line\"] += 1\n",
    "    if lnode.tag == 'gap':\n",
    "        line = {key:meta_d[key] for key in [\"id_text\", \"text_name\", \"version\", \"id_line\"]}\n",
    "        line[\"extent\"] = lnode.xpath(\"string(@extent)\")\n",
    "        alltexts.append(line)\n",
    "        return\n",
    "    \n",
    "    for node in lnode.xpath('.//w|.//gloss[@lang=\"akk\"]'):\n",
    "                        # get <w> nodes and <gloss> nodes, but only Akkadian glosses\n",
    "        getword(node)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.9 Sections\n",
    "\n",
    "Some [ETCSL](http://etcsl.orinst.ox.ac.uk) compositions are divided into **sections**. That is the case, in particular, when a composition has gaps of unknown length. \n",
    "\n",
    "The function `getsection()` is called by `getversion()` and receives one argument: `tree` (the `etree` object representing one version of the composition). The function updates `meta_d`, a dictionary of meta data. The function `getsection()` checks to see whether a sub-division into sections is present. If so, it iterates over these sections. Each section (or, if there are no sections, the composition/version as a whole) consists of series of lines and/or gaps. The function `getline()` is called to process each line or gap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsection(tree):\n",
    "    sections = tree.xpath('.//div1')\n",
    "    \n",
    "    if len(sections) > 0: # if the text is not divided into sections - skip to else:\n",
    "        for snode in sections:\n",
    "            section = snode.xpath('string(@n)')\n",
    "            for lnode in snode.xpath('.//l|.//gap'):\n",
    "                if lnode.tag == 'l':\n",
    "                    line = section + lnode.xpath('string(@n)')\n",
    "                    meta_d[\"label\"] = line   # \"label\" is the human-legible \n",
    "                getline(lnode)\n",
    "\n",
    "    else:\n",
    "        for lnode in tree.xpath('.//l|.//gap'):\n",
    "            if lnode.tag == 'l':\n",
    "                line_no = lnode.xpath('string(@n)')\n",
    "                meta_d[\"label\"] = line_no\n",
    "            getline(lnode)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.10 Versions\n",
    "\n",
    "In some cases an [ETCSL](http://etcsl.orinst.ox.ac.uk) file contains different versions of the same composition. The versions may be distinguished as 'Version A' vs. 'Version B' or may indicate the provenance of the version ('A version from Urim' vs. 'A version from Nibru'). In the edition of the proverbs the same mechanism is used to distinguish between numerous tablets (often lentils) that contain just one proverb, or a few, and are collected in the files \"Proverbs from Susa,\" \"Proverbs from Nibru,\" etc. ([ETCSL](http://etcsl.orinst.ox.ac.uk) c.6.2.1 - c.6.2.5).\n",
    "\n",
    "The function `getversion()` is called by the function `parsetext()` and receives one argument: `tree` (the `etree` object). The function updates`meta_d`, a dictionary of meta-data. The function checks to see if versions are available in the file that is being parsed. If so, the function iterates over these versions while adding the version name to the `meta_d` dictionary. If there are no versions, the version name is left empty. The parsing process is continued by calling `getsection()` to see if the composition/version is further divided into sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getversion(tree):\n",
    "    versions = tree.xpath('.//body[child::head]')\n",
    "\n",
    "    if len(versions) > 0: # if the text is not divided into versions - skip 'getversion()':\n",
    "        for vnode in versions:\n",
    "            version = vnode.xpath('string(head)')\n",
    "            version = eq[\"versions\"][version]\n",
    "            meta_d[\"version\"] = version\n",
    "            getsection(vnode)\n",
    "\n",
    "    else:\n",
    "        meta_d[\"version\"] = ''\n",
    "        getsection(tree)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.11 Parse a Text\n",
    "\n",
    "The function `parsetext()` takes one xml file (a composition in [ETCSL](http://etcsl.orinst.ox.ac.uk)) and parses it, calling a variety of functions defined above. \n",
    "\n",
    "The parsing is done by the `etree` package in the `lxml` library. Before the file can be parsed properly the so-called HTML entities need to be replaced by their Unicode equivalents. This is done by calling the `ampersands()` function (see above, section 3: Preprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsetext(file):\n",
    "    with open(f'etcsl/transliterations/{file}') as f:\n",
    "        xmltext = f.read()\n",
    "    xmltext = ampersands(xmltext)          #replace HTML entities by Unicode equivalents\n",
    "    \n",
    "    tree = etree.fromstring(xmltext)\n",
    "    \n",
    "    tree = mark_extra(tree, \"additional\") # mark additional words with attribute status = 'additional'\n",
    "    tree = mark_extra(tree, \"secondary\")  # mark secondary words with attribute status = 'secondary'\n",
    "    name = tree.xpath('string(//title)')\n",
    "    name = name.replace(' -- a composite transliteration', '')\n",
    "    name = name.replace(',', '')\n",
    "    meta_d[\"id_text\"] =  file[:-4]\n",
    "    meta_d[\"text_name\"] = name\n",
    "    meta_d[\"id_line\"] = 0\n",
    "    getversion(tree)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.12 Main Process\n",
    "\n",
    "The list `alltexts` is created as an empty list. It will be filled with dictionaries, each dictionary representing one word form.\n",
    "\n",
    "The variable `textlist` is a list of all the `XML` files with [ETCSL](http://etcsl.orinst.ox.ac.uk) compositions in the directory `etcsl/transliterations`. Each file  is sent as an argument to the function `parsetext()`. \n",
    "\n",
    "The dictionary `meta_d` is created as an empty dictionary. On each level of analysis the dictionary is updated with meta-data, such as text ID, version name, line number, etc.\n",
    "\n",
    "The list is transformed into a `pandas` DataFrame. All missing values (`NaN`) are replaced by empty strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9261a119334c4994618a9817f03cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "textlist = os.listdir('etcsl/transliterations')\n",
    "textlist.sort()\n",
    "\n",
    "amp = re.compile(r'&[^;]+;') #regex for HTML entities, used in ampersands()\n",
    "\n",
    "asccj, unicj = 'cjCJ', 'šŋŠŊ'\n",
    "transcj = str.maketrans(asccj, unicj) # translation table for c > š and j > ŋ\n",
    "\n",
    "ind = re.compile(r'[a-zŋḫṣšṭA-ZŊḪṢŠṬ][0-9x]{1,2}') #regex for sign index nos preceded by a letter\n",
    "ascind, uniind = '0123456789x', '₀₁₂₃₄₅₆₇₈₉ₓ'\n",
    "transind = str.maketrans(ascind, uniind) # translation table for index numbers\n",
    "# regex ind and the translation tables transind and transcj are used in tounicode()\n",
    "\n",
    "alltexts = []\n",
    "files = tqdm(textlist)\n",
    "for file in files:\n",
    "    files.set_description(f'ETCSL {file[2:-4]}')\n",
    "    meta_d = {}\n",
    "    parsetext(file)\n",
    "\n",
    "df = pd.DataFrame(alltexts).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>text_name</th>\n",
       "      <th>id_line</th>\n",
       "      <th>version</th>\n",
       "      <th>label</th>\n",
       "      <th>cf</th>\n",
       "      <th>gw</th>\n",
       "      <th>pos</th>\n",
       "      <th>form</th>\n",
       "      <th>lang</th>\n",
       "      <th>extent</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c.0.1.1</td>\n",
       "      <td>Ur III catalogue from Nibru (N1)</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>dubsaŋ</td>\n",
       "      <td>first</td>\n",
       "      <td>AJ</td>\n",
       "      <td>dub-saŋ-ta</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c.0.1.1</td>\n",
       "      <td>Ur III catalogue from Nibru (N1)</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>Enki</td>\n",
       "      <td>1</td>\n",
       "      <td>DN</td>\n",
       "      <td>{d}en-ki</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c.0.1.1</td>\n",
       "      <td>Ur III catalogue from Nibru (N1)</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>unu</td>\n",
       "      <td>dwelling</td>\n",
       "      <td>N</td>\n",
       "      <td>unu₂</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c.0.1.1</td>\n",
       "      <td>Ur III catalogue from Nibru (N1)</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>gal</td>\n",
       "      <td>big</td>\n",
       "      <td>V/i</td>\n",
       "      <td>gal</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c.0.1.1</td>\n",
       "      <td>Ur III catalogue from Nibru (N1)</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>ed</td>\n",
       "      <td>ascend</td>\n",
       "      <td>V/i</td>\n",
       "      <td>im-ed₃</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170851</th>\n",
       "      <td>c.6.2.5</td>\n",
       "      <td>Proverbs: of unknown provenance</td>\n",
       "      <td>211</td>\n",
       "      <td>YBC 9912</td>\n",
       "      <td>1</td>\n",
       "      <td>haš</td>\n",
       "      <td>thigh</td>\n",
       "      <td>N</td>\n",
       "      <td>haš₂</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170852</th>\n",
       "      <td>c.6.2.5</td>\n",
       "      <td>Proverbs: of unknown provenance</td>\n",
       "      <td>211</td>\n",
       "      <td>YBC 9912</td>\n",
       "      <td>1</td>\n",
       "      <td>gid</td>\n",
       "      <td>long</td>\n",
       "      <td>V/i</td>\n",
       "      <td>ba-ra-an-gid₂-nam</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170853</th>\n",
       "      <td>c.6.2.5</td>\n",
       "      <td>Proverbs: of unknown provenance</td>\n",
       "      <td>211</td>\n",
       "      <td>YBC 9912</td>\n",
       "      <td>1</td>\n",
       "      <td>lu</td>\n",
       "      <td>person</td>\n",
       "      <td>N</td>\n",
       "      <td>lu₂</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170854</th>\n",
       "      <td>c.6.2.5</td>\n",
       "      <td>Proverbs: of unknown provenance</td>\n",
       "      <td>211</td>\n",
       "      <td>YBC 9912</td>\n",
       "      <td>1</td>\n",
       "      <td>ŋeši</td>\n",
       "      <td>sesame</td>\n",
       "      <td>N</td>\n",
       "      <td>še-ŋiš-i₃</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170855</th>\n",
       "      <td>c.6.2.5</td>\n",
       "      <td>Proverbs: of unknown provenance</td>\n",
       "      <td>211</td>\n",
       "      <td>YBC 9912</td>\n",
       "      <td>1</td>\n",
       "      <td>il</td>\n",
       "      <td>raise</td>\n",
       "      <td>V/t</td>\n",
       "      <td>bi₂-ib₂-il₂-il₂</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170856 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_text                         text_name  id_line   version label  \\\n",
       "0       c.0.1.1  Ur III catalogue from Nibru (N1)        1               1   \n",
       "1       c.0.1.1  Ur III catalogue from Nibru (N1)        2               2   \n",
       "2       c.0.1.1  Ur III catalogue from Nibru (N1)        2               2   \n",
       "3       c.0.1.1  Ur III catalogue from Nibru (N1)        2               2   \n",
       "4       c.0.1.1  Ur III catalogue from Nibru (N1)        2               2   \n",
       "...         ...                               ...      ...       ...   ...   \n",
       "170851  c.6.2.5   Proverbs: of unknown provenance      211  YBC 9912     1   \n",
       "170852  c.6.2.5   Proverbs: of unknown provenance      211  YBC 9912     1   \n",
       "170853  c.6.2.5   Proverbs: of unknown provenance      211  YBC 9912     1   \n",
       "170854  c.6.2.5   Proverbs: of unknown provenance      211  YBC 9912     1   \n",
       "170855  c.6.2.5   Proverbs: of unknown provenance      211  YBC 9912     1   \n",
       "\n",
       "            cf        gw  pos               form lang extent status  \n",
       "0       dubsaŋ     first   AJ         dub-saŋ-ta  sux                \n",
       "1         Enki         1   DN           {d}en-ki  sux                \n",
       "2          unu  dwelling    N               unu₂  sux                \n",
       "3          gal       big  V/i                gal  sux                \n",
       "4           ed    ascend  V/i             im-ed₃  sux                \n",
       "...        ...       ...  ...                ...  ...    ...    ...  \n",
       "170851     haš     thigh    N               haš₂  sux                \n",
       "170852     gid      long  V/i  ba-ra-an-gid₂-nam  sux                \n",
       "170853      lu    person    N                lu₂  sux                \n",
       "170854    ŋeši    sesame    N          še-ŋiš-i₃  sux                \n",
       "170855      il     raise  V/t    bi₂-ib₂-il₂-il₂  sux                \n",
       "\n",
       "[170856 rows x 12 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.13 Save as CSV\n",
    "The DataFrame is saved as a `CSV` file named `alltexts.csv` in the directory `output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/alltexts.csv', 'w', encoding=\"utf-8\") as w:\n",
    "    df.to_csv(w, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
