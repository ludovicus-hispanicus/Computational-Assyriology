{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.3 Extract Lemmatization from ORACC JSON: Basic Parser\n",
    "The code in this notebook will parse [ORACC](http://oracc.org) `JSON` files to extract lemmatization data for one or more projects. The resulting `csv` (Comma Separated Values) file is named `parsed.csv` and has two fields: a Text ID (e.g. `dcclt/Q000039`) and a string of lemmas in the format `lugal[king]N` (or `Å¡arru[king]N` for Akkadian texts).\n",
    "\n",
    "The output of the Basic Parser contains *only* text IDs and lemmas. This format is useful for so-called [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) techniques such as word clouds or [topic modeling](https://en.wikipedia.org/wiki/Topic_model). The `JSON` files, however, contain a wealth of other data, including language (Sumerian, Akkadian, Emesal, etc.), orthographic form, morphology (currently only for Sumerian and Emesal), line numbers, breakage, etc. The extended JSON parser notebook (2.1.4), building upon the techniques demonstrated here, will show how to extract any type of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3.0 Import Packages\n",
    "* pandas: data analysis and manipulation; dataframes\n",
    "* ipywidgets: user interface (enter project names)\n",
    "* panel: another set of widget tools (here used for displaying a JSON file)\n",
    "* zipfile: read data from a zipped file\n",
    "* json: read a json object\n",
    "* tqdm: progress bar\n",
    "* os: basic Operating System tasks (such as creating a directory)\n",
    "* sys: change system parameters\n",
    "* utils: compass-specific utilities (download files from ORACC, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, js_modules, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "    if (js_modules == null) js_modules = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls.length === 0 && js_modules.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      window.requirejs.config({'paths': {'tabulator': 'https://unpkg.com/tabulator-tables@4.9.3/dist/js/tabulator'}});\n",
       "      require([\"tabulator\"], function(Tabulator,) {\n",
       "        window.Tabulator = Tabulator;\n",
       "      })\n",
       "    }\n",
       "    if (((window['tabulator'] !== undefined) && (!(window['tabulator'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://unpkg.com/tabulator-tables@4.9.3/dist/js/tabulator.js', 'https://unpkg.com/moment@2.27.0/moment.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (var i = 0; i < js_modules.length; i++) {\n",
       "      var url = js_modules[i];\n",
       "      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    if (!js_urls.length && !js_modules.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://unpkg.com/tabulator-tables@4.9.3/dist/js/tabulator.js\", \"https://unpkg.com/moment@2.27.0/moment.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\", \"https://unpkg.com/@holoviz/panel@^0.11.3/dist/panel.min.js\"];\n",
       "  var js_modules = [];\n",
       "  var css_urls = [\"https://unpkg.com/tabulator-tables@4.9.3/dist/css/tabulator_simple.min.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/dataframe.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/loading.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/widgets.css\"];\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\n    .bk.pn-loading.arcs:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiBzdHlsZT0ibWFyZ2luOiBhdXRvOyBiYWNrZ3JvdW5kOiBub25lOyBkaXNwbGF5OiBibG9jazsgc2hhcGUtcmVuZGVyaW5nOiBhdXRvOyIgdmlld0JveD0iMCAwIDEwMCAxMDAiIHByZXNlcnZlQXNwZWN0UmF0aW89InhNaWRZTWlkIj4gIDxjaXJjbGUgY3g9IjUwIiBjeT0iNTAiIHI9IjMyIiBzdHJva2Utd2lkdGg9IjgiIHN0cm9rZT0iI2MzYzNjMyIgc3Ryb2tlLWRhc2hhcnJheT0iNTAuMjY1NDgyNDU3NDM2NjkgNTAuMjY1NDgyNDU3NDM2NjkiIGZpbGw9Im5vbmUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCI+ICAgIDxhbmltYXRlVHJhbnNmb3JtIGF0dHJpYnV0ZU5hbWU9InRyYW5zZm9ybSIgdHlwZT0icm90YXRlIiByZXBlYXRDb3VudD0iaW5kZWZpbml0ZSIgZHVyPSIxcyIga2V5VGltZXM9IjA7MSIgdmFsdWVzPSIwIDUwIDUwOzM2MCA1MCA1MCI+PC9hbmltYXRlVHJhbnNmb3JtPiAgPC9jaXJjbGU+PC9zdmc+\\\")\\n    }\\n    \");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, js_modules, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'paths': {'tabulator': 'https://unpkg.com/tabulator-tables@4.9.3/dist/js/tabulator'}});\n      require([\"tabulator\"], function(Tabulator,) {\n        window.Tabulator = Tabulator;\n      })\n    }\n    if (((window['tabulator'] !== undefined) && (!(window['tabulator'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://unpkg.com/tabulator-tables@4.9.3/dist/js/tabulator.js', 'https://unpkg.com/moment@2.27.0/moment.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://unpkg.com/tabulator-tables@4.9.3/dist/js/tabulator.js\", \"https://unpkg.com/moment@2.27.0/moment.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\", \"https://unpkg.com/@holoviz/panel@^0.11.3/dist/panel.min.js\"];\n  var js_modules = [];\n  var css_urls = [\"https://unpkg.com/tabulator-tables@4.9.3/dist/css/tabulator_simple.min.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/dataframe.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/loading.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/widgets.css\"];\n  var inline_js = [\n    function(Bokeh) {\n      inject_raw_css(\"\\n    .bk.pn-loading.arcs:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiBzdHlsZT0ibWFyZ2luOiBhdXRvOyBiYWNrZ3JvdW5kOiBub25lOyBkaXNwbGF5OiBibG9jazsgc2hhcGUtcmVuZGVyaW5nOiBhdXRvOyIgdmlld0JveD0iMCAwIDEwMCAxMDAiIHByZXNlcnZlQXNwZWN0UmF0aW89InhNaWRZTWlkIj4gIDxjaXJjbGUgY3g9IjUwIiBjeT0iNTAiIHI9IjMyIiBzdHJva2Utd2lkdGg9IjgiIHN0cm9rZT0iI2MzYzNjMyIgc3Ryb2tlLWRhc2hhcnJheT0iNTAuMjY1NDgyNDU3NDM2NjkgNTAuMjY1NDgyNDU3NDM2NjkiIGZpbGw9Im5vbmUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCI+ICAgIDxhbmltYXRlVHJhbnNmb3JtIGF0dHJpYnV0ZU5hbWU9InRyYW5zZm9ybSIgdHlwZT0icm90YXRlIiByZXBlYXRDb3VudD0iaW5kZWZpbml0ZSIgZHVyPSIxcyIga2V5VGltZXM9IjA7MSIgdmFsdWVzPSIwIDUwIDUwOzM2MCA1MCA1MCI+PC9hbmltYXRlVHJhbnNmb3JtPiAgPC9jaXJjbGU+PC9zdmc+\\\")\\n    }\\n    \");\n    },\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, js_modules, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import zipfile\n",
    "import panel as pn\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import sys\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "import utils\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3.1 Create Directories, if Necessary\n",
    "The two directories needed for this script are `jsonzip` and `output`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('jsonzip', exist_ok = True)\n",
    "os.makedirs('output', exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3.2 Input Project Names\n",
    "We can download and manipulate multiple [ORACC](http://oracc.org) `zip` files at the same time. The `Textarea` widget provides a space for typing project abbreviations, separated by a comma. The widget is assigned to the variable `projects`. The text entered in the `Textarea` widget can be retrieved as `projects.value`.\n",
    "\n",
    ":::{warning}\n",
    "Subprojects must be listed separately, they are not included in the main project. A subproject is named `[PROJECT]/[SUBPROJECT]`, for instance `saao/saa01`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961f4ef33da84cfab23c0b5e27d2d3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='obmc', description='Projects:', placeholder='Type project names, separated by commas')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "projects = widgets.Textarea(\n",
    "    value=\"obmc\",\n",
    "    placeholder='Type project names, separated by commas',\n",
    "    description='Projects:',\n",
    ")\n",
    "projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3.3 Split the List of Projects and Download the ZIP files.\n",
    "Use the `format_project_list()` and `oracc_download()` functions from the `utils` module to download the requested projects. The code of these function is discussed in more detail in 2.1.0. Download ORACC JSON Files. The function returns a new version of the project list, with duplicates and non-existing projects removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving http://build-oracc.museum.upenn.edu/json/dcclt.zip as jsonzip/dcclt.zip.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2a5e438f804ee1b5584bdc94174f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dcclt:   0%|          | 0.00/71.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "project_list = utils.format_project_list(projects.value)\n",
    "project_list = utils.oracc_download(project_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3.4 ORACC JSON: the CDL Structure\n",
    "[ORACC](http://oracc.org) `zip` files contain JSON files for each individual text (exemplar or composite) that is part of that project. These files are found in the directory `corpusjson` and are named after the text ID (for instance `P200931.json`). The `parse_text_json()` function (2.1.3.5) extracts basic lemmatization data from each of those files. In the next section (2.1.4) we will review techniques to extract other types of information from these same files.\n",
    "\n",
    "The structure of the JSON files for text editions, is fairly complex, because of the hierarchical structure of textual data. A text may have one or more surfaces (obverse, reverse), each surface may have one or more columns; each column has lines; each line has words; and each word has signs.\n",
    "\n",
    "\ttext object\n",
    "\t\tsurface\n",
    "\t\t\tcolumn\n",
    "\t\t\t\tline\n",
    "\t\t\t\t\tword\n",
    "\t\t\t\t\t\tsign\n",
    "\n",
    "How many of those layers are present in a particular text is impossible to predict. Some tablets have columns, others do not; most surfaces have text, but not all surfaces do. In addition, a text consists of textual units, such as paragraphs, sentences, and phrases.\n",
    "\n",
    "\ttext\n",
    "\t\tsentence\n",
    "\t\t\tphrase\n",
    "\t\t\t\tword\n",
    "\n",
    "These two hierarchies, which do not necessarily coincide, are simultaneously represented in the JSON tree. The object-oriented hierarchy is represented by d (Discontinuity) nodes, the discourse-oriented hierarchy is represented by c (Chunk) nodes.\n",
    "\n",
    "The [ORACC](http://oracc.org) JSON tree for a text edition consists of a hierarchy of `cdl` keys. The name `cdl` is based on the three main components of the nested tree: Chunks (text units), Discontinuities (physical units), and Lemmas (words). A Chunk is a chunk of text of any length: the body of the text, a sentence, a phrase, a word, etc. A Discontinuity is the text object, the obverse or reverse, the beginning of a column, a break in the text, a horizontal ruling on the tablet, or the beginning of a new line. A Lemma is the lemmatization of a single word in the text, including the information on the sign level. \n",
    "\n",
    "The value of a `cdl` key is a list of one or more dictionaries. Each of these dictionaries contains the key \"node\" which may have the values \"c\" (for Chunk), \"d\" (for Discontinuity), or \"l\" (for Lemma). Any \"c\" dictionary may contain a further `cdl` key, which again has as its value a list of dictionaries of the \"c\", \"d\", or \"l\" type. The \"c\" and \"d\" dictionaries always have a key `type`, which may have values such as `text`, `discourse`, or `sentence` (for \"c\" nodes) or `object`, `obverse`, `column-start`, `line-start` (for \"d\" nodes). Discontinuities (or \"d\" nodes\") may also have `type : nonx`, recording non-textual physical features such as rulings on the tablet, or missing lines. An \"l\" (Lemma) dictionary, is always at the bottom of the `cdl` hierarchy. The \"l\" dictionary itself contains an \"f\" key which has as its value a dictionary that contains all the lemmatization data of a single word (transliteration, citation form, guide word, part of speech, etc.). The \"f\" dictionary includes a \"gdl\" key (for Grapheme Description Language), which identifies the graphemes (cuneiform signs) of which the word is composed, with information on the reading and the function (syllabogram, logogram, determinative, etc.) of those graphemes (in some [ORACC](http://oracc.org) projects the `gdl` node will include the Unicode representation of cuneiform signs themselves as well).\n",
    "\n",
    "The structure may be illustrated with the beginning of [P251867](http://oracc.org/dcclt/P251867), an Old Babylonian 3-line lentil (school text), edited in [DCCLT](http://oracc.org/dcclt) (click on the arrow the left of the `cdl` key to expose the hierarchy): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='1002'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"6dc428b3-a63e-4f1f-833b-2a7b2f1f0732\" data-root-id=\"1002\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    var docs_json = {\"8b498edb-b876-45c3-87a0-ddd4b4e48cd2\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"reload\":false},\"id\":\"1004\",\"type\":\"panel.models.location.Location\"},{\"attributes\":{\"height\":300,\"margin\":[5,20,5,5],\"name\":\"JSON\",\"sizing_mode\":\"fixed\",\"text\":\"{\\\"type\\\": \\\"cdl\\\", \\\"project\\\": \\\"dcclt\\\", \\\"source\\\": \\\"http://oracc.org/dcclt\\\", \\\"license\\\": \\\"This data is released under the CC0 license\\\", \\\"license-url\\\": \\\"https://creativecommons.org/publicdomain/zero/1.0/\\\", \\\"more-info\\\": \\\"http://oracc.org/doc/opendata/\\\", \\\"UTC-timestamp\\\": \\\"2021-06-19T00:53:51\\\", \\\"textid\\\": \\\"P251867\\\", \\\"cdl\\\": [{\\\"node\\\": \\\"c\\\", \\\"type\\\": \\\"text\\\", \\\"id\\\": \\\"P251867.U0\\\", \\\"cdl\\\": [{\\\"node\\\": \\\"d\\\", \\\"type\\\": \\\"object\\\", \\\"ref\\\": \\\"\\\"}, {\\\"node\\\": \\\"d\\\", \\\"subtype\\\": \\\"obverse\\\", \\\"type\\\": \\\"surface\\\", \\\"ref\\\": \\\"P251867.o.1\\\", \\\"label\\\": \\\"o\\\"}, {\\\"node\\\": \\\"c\\\", \\\"type\\\": \\\"discourse\\\", \\\"subtype\\\": \\\"body\\\", \\\"id\\\": \\\"P251867.U1\\\", \\\"cdl\\\": [{\\\"node\\\": \\\"c\\\", \\\"type\\\": \\\"sentence\\\", \\\"implicit\\\": \\\"yes\\\", \\\"id\\\": \\\"P251867.U2\\\", \\\"label\\\": \\\"o 1\\\", \\\"cdl\\\": [{\\\"node\\\": \\\"d\\\", \\\"type\\\": \\\"line-start\\\", \\\"ref\\\": \\\"P251867.2\\\", \\\"n\\\": \\\"1\\\", \\\"label\\\": \\\"o 1\\\"}, {\\\"node\\\": \\\"l\\\", \\\"frag\\\": \\\"{\\\\u014be\\\\u0161}ma\\\\u2082-durah-abzu\\\", \\\"id\\\": \\\"P251867.l1cd5e\\\", \\\"ref\\\": \\\"P251867.2.1\\\", \\\"inst\\\": \\\"Madurahabzu[1]ON\\\", \\\"exosig\\\": \\\"@dcclt%sux:{\\\\u014be\\\\u0161}ma\\\\u2082-durah-abzu=Madurahabzu[1//1]ON'ON$Madurahabzu/{\\\\u014be\\\\u0161}ma\\\\u2082-durah-abzu#~\\\", \\\"exoprj\\\": \\\"epsd2/names\\\", \\\"exolng\\\": \\\"sux\\\", \\\"f\\\": {\\\"lang\\\": \\\"sux\\\", \\\"form\\\": \\\"{\\\\u014be\\\\u0161}ma\\\\u2082-durah-abzu\\\", \\\"gdl\\\": [{\\\"det\\\": \\\"semantic\\\", \\\"pos\\\": \\\"pre\\\", \\\"seq\\\": [{\\\"v\\\": \\\"\\\\u014be\\\\u0161\\\", \\\"id\\\": \\\"P251867.2.1.0\\\"}]}, {\\\"v\\\": \\\"ma\\\\u2082\\\", \\\"id\\\": \\\"P251867.2.1.1\\\", \\\"delim\\\": \\\"-\\\"}, {\\\"v\\\": \\\"durah\\\", \\\"id\\\": \\\"P251867.2.1.2\\\", \\\"queried\\\": \\\"1\\\", \\\"delim\\\": \\\"-\\\"}, {\\\"v\\\": \\\"abzu\\\", \\\"id\\\": \\\"P251867.2.1.3\\\"}], \\\"cf\\\": \\\"Madurahabzu\\\", \\\"gw\\\": \\\"1\\\", \\\"sense\\\": \\\"1\\\", \\\"norm0\\\": \\\"Madurahabzu\\\", \\\"pos\\\": \\\"ON\\\", \\\"epos\\\": \\\"ON\\\", \\\"base\\\": \\\"{\\\\u014be\\\\u0161}ma\\\\u2082-durah-abzu\\\", \\\"morph\\\": \\\"~\\\"}, \\\"props\\\": [{\\\"name\\\": \\\"field\\\", \\\"group\\\": \\\"env\\\", \\\"ngram\\\": \\\"-1\\\", \\\"ref\\\": \\\"P251867.2.1\\\"}, {\\\"name\\\": \\\"discourse\\\", \\\"group\\\": \\\"env\\\", \\\"value\\\": \\\"body\\\", \\\"ngram\\\": \\\"-1\\\", \\\"ref\\\": \\\"P251867.2.1\\\"}]}]}, {\\\"node\\\": \\\"c\\\", \\\"type\\\": \\\"sentence\\\", \\\"implicit\\\": \\\"yes\\\", \\\"tag\\\": \\\"#auto\\\", \\\"id\\\": \\\"P251867.U4\\\", \\\"label\\\": \\\"o 2\\\", \\\"cdl\\\": [{\\\"node\\\": \\\"d\\\", \\\"type\\\": \\\"line-start\\\", \\\"ref\\\": \\\"P251867.3\\\", \\\"n\\\": \\\"2\\\", \\\"label\\\": \\\"o 2\\\"}, {\\\"node\\\": \\\"l\\\", \\\"frag\\\": \\\"{\\\\u014be\\\\u0161}ma\\\\u2082-a\\\\u0161-te\\\", \\\"id\\\": \\\"P251867.l1cd5f\\\", \\\"ref\\\": \\\"P251867.3.1\\\", \\\"inst\\\": \\\"Ma\\\\u02bea\\\\u0161te[1]ON\\\", \\\"exosig\\\": \\\"@dcclt%sux:{\\\\u014be\\\\u0161}ma\\\\u2082-a\\\\u0161-te=Ma\\\\u02bea\\\\u0161te[1//1]ON'ON$Ma\\\\u02bea\\\\u0161te/{\\\\u014be\\\\u0161}ma\\\\u2082-a\\\\u0161-te#~\\\", \\\"exoprj\\\": \\\"epsd2/names\\\", \\\"exolng\\\": \\\"sux\\\", \\\"f\\\": {\\\"lang\\\": \\\"sux\\\", \\\"form\\\": \\\"{\\\\u014be\\\\u0161}ma\\\\u2082-a\\\\u0161-te\\\", \\\"gdl\\\": [{\\\"det\\\": \\\"semantic\\\", \\\"pos\\\": \\\"pre\\\", \\\"seq\\\": [{\\\"v\\\": \\\"\\\\u014be\\\\u0161\\\", \\\"id\\\": \\\"P251867.3.1.0\\\"}]}, {\\\"v\\\": \\\"ma\\\\u2082\\\", \\\"id\\\": \\\"P251867.3.1.1\\\", \\\"delim\\\": \\\"-\\\"}, {\\\"v\\\": \\\"a\\\\u0161\\\", \\\"id\\\": \\\"P251867.3.1.2\\\", \\\"delim\\\": \\\"-\\\"}, {\\\"v\\\": \\\"te\\\", \\\"id\\\": \\\"P251867.3.1.3\\\"}], \\\"cf\\\": \\\"Ma\\\\u02bea\\\\u0161te\\\", \\\"gw\\\": \\\"1\\\", \\\"sense\\\": \\\"1\\\", \\\"norm0\\\": \\\"Ma\\\\u02bea\\\\u0161te\\\", \\\"pos\\\": \\\"ON\\\", \\\"epos\\\": \\\"ON\\\", \\\"base\\\": \\\"{\\\\u014be\\\\u0161}ma\\\\u2082-a\\\\u0161-te\\\", \\\"morph\\\": \\\"~\\\"}, \\\"props\\\": [{\\\"name\\\": \\\"field\\\", \\\"group\\\": \\\"env\\\", \\\"ngram\\\": \\\"-1\\\", \\\"ref\\\": \\\"P251867.3.1\\\"}, {\\\"name\\\": \\\"discourse\\\", \\\"group\\\": \\\"env\\\", \\\"value\\\": \\\"body\\\", \\\"ngram\\\": \\\"-1\\\", \\\"ref\\\": \\\"P251867.3.1\\\"}]}]}, {\\\"node\\\": \\\"c\\\", \\\"type\\\": \\\"sentence\\\", \\\"implicit\\\": \\\"yes\\\", \\\"tag\\\": \\\"#auto\\\", \\\"id\\\": \\\"P251867.U6\\\", \\\"label\\\": \\\"o 3\\\", \\\"cdl\\\": [{\\\"node\\\": \\\"d\\\", \\\"type\\\": \\\"line-start\\\", \\\"ref\\\": \\\"P251867.4\\\", \\\"n\\\": \\\"3\\\", \\\"label\\\": \\\"o 3\\\"}, {\\\"node\\\": \\\"l\\\", \\\"frag\\\": \\\"{\\\\u014be\\\\u0161}ma\\\\u2082-kar-nam-nun-ta-\\\\u2e22e\\\\u2083-a\\\\u2e23\\\", \\\"id\\\": \\\"P251867.l1cd60\\\", \\\"ref\\\": \\\"P251867.4.1\\\", \\\"inst\\\": \\\"Makarnunta\\\\u02beea[]\\\", \\\"exosig\\\": \\\"@dcclt%sux:{\\\\u014be\\\\u0161}ma\\\\u2082-kar-nam-nun-ta-e\\\\u2083-a=Makarnunta\\\\u02beea[1//1]ON'ON$Makarnunta\\\\u02beea/{\\\\u014be\\\\u0161}ma\\\\u2082-kar-nam-nun-ta-e\\\\u2083-a#~\\\", \\\"exoprj\\\": \\\"epsd2/names\\\", \\\"exolng\\\": \\\"sux\\\", \\\"f\\\": {\\\"lang\\\": \\\"sux\\\", \\\"form\\\": \\\"{\\\\u014be\\\\u0161}ma\\\\u2082-kar-nam-nun-ta-e\\\\u2083-a\\\", \\\"gdl\\\": [{\\\"det\\\": \\\"semantic\\\", \\\"pos\\\": \\\"pre\\\", \\\"seq\\\": [{\\\"v\\\": \\\"\\\\u014be\\\\u0161\\\", \\\"id\\\": \\\"P251867.4.1.0\\\"}]}, {\\\"v\\\": \\\"ma\\\\u2082\\\", \\\"id\\\": \\\"P251867.4.1.1\\\", \\\"delim\\\": \\\"-\\\"}, {\\\"v\\\": \\\"kar\\\", \\\"id\\\": \\\"P251867.4.1.2\\\", \\\"delim\\\": \\\"-\\\"}, {\\\"v\\\": \\\"nam\\\", \\\"id\\\": \\\"P251867.4.1.3\\\", \\\"delim\\\": \\\"-\\\"}, {\\\"v\\\": \\\"nun\\\", \\\"id\\\": \\\"P251867.4.1.4\\\", \\\"delim\\\": \\\"-\\\"}, {\\\"v\\\": \\\"ta\\\", \\\"id\\\": \\\"P251867.4.1.5\\\", \\\"delim\\\": \\\"-\\\"}, {\\\"v\\\": \\\"e\\\\u2083\\\", \\\"id\\\": \\\"P251867.4.1.6\\\", \\\"break\\\": \\\"damaged\\\", \\\"ho\\\": \\\"1\\\", \\\"delim\\\": \\\"-\\\"}, {\\\"v\\\": \\\"a\\\", \\\"id\\\": \\\"P251867.4.1.7\\\", \\\"break\\\": \\\"damaged\\\", \\\"hc\\\": \\\"1\\\"}], \\\"cf\\\": \\\"Makarnunta\\\\u02beea\\\", \\\"gw\\\": \\\"1\\\", \\\"sense\\\": \\\"1\\\", \\\"norm0\\\": \\\"Makarnunta\\\\u02beea\\\", \\\"pos\\\": \\\"ON\\\", \\\"epos\\\": \\\"ON\\\", \\\"base\\\": \\\"{\\\\u014be\\\\u0161}ma\\\\u2082-kar-nam-nun-ta-e\\\\u2083-a\\\", \\\"morph\\\": \\\"~\\\"}, \\\"props\\\": [{\\\"name\\\": \\\"field\\\", \\\"group\\\": \\\"env\\\", \\\"ngram\\\": \\\"-1\\\", \\\"ref\\\": \\\"P251867.4.1\\\"}, {\\\"name\\\": \\\"discourse\\\", \\\"group\\\": \\\"env\\\", \\\"value\\\": \\\"body\\\", \\\"ngram\\\": \\\"-1\\\", \\\"ref\\\": \\\"P251867.4.1\\\"}]}]}]}]}]}\",\"theme\":\"light\",\"width\":500},\"id\":\"1002\",\"type\":\"panel.models.markup.JSON\"},{\"attributes\":{\"client_comm_id\":\"456c83cdbedd4feab4100da7e00dcf3c\",\"comm_id\":\"d8e39b106e4044dd8e5b62e90fb2b50f\",\"plot_id\":\"1002\"},\"id\":\"1003\",\"type\":\"panel.models.comm_manager.CommManager\"}],\"root_ids\":[\"1002\",\"1003\",\"1004\"]},\"title\":\"Bokeh Application\",\"version\":\"2.3.2\"}};\n",
       "    var render_items = [{\"docid\":\"8b498edb-b876-45c3-87a0-ddd4b4e48cd2\",\"root_ids\":[\"1002\"],\"roots\":{\"1002\":\"6dc428b3-a63e-4f1f-833b-2a7b2f1f0732\"}}];\n",
       "    root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       "JSON(dict, height=300, sizing_mode='fixed', theme='light', width=500)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "1002"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('P251867.json', 'r', encoding='utf-8') as j:\n",
    "    json_obj = json.load(j)\n",
    "json = pn.pane.JSON(json_obj, name='JSON', height=300, width=500, theme='light')\n",
    "json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\"cdl\": [\n",
    "{\n",
    "  \"node\": \"c\",\n",
    "  \"type\": \"text\",\n",
    "  \"id\": \"P251867.U0\",\n",
    "  \"cdl\": [\n",
    "    {\n",
    "      \"node\": \"d\",\n",
    "      \"type\": \"object\",\n",
    "      \"ref\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"node\": \"d\",\n",
    "      \"subtype\": \"obverse\",\n",
    "      \"type\": \"obverse\",\n",
    "      \"ref\": \"P251867.o.1\",\n",
    "      \"label\": \"o\"\n",
    "    },\n",
    "    {\n",
    "      \"node\": \"c\",\n",
    "      \"type\": \"discourse\",\n",
    "      \"subtype\": \"body\",\n",
    "      \"id\": \"P251867.U1\",\n",
    "      \"cdl\": [\n",
    "        {\n",
    "          \"node\": \"c\",\n",
    "          \"type\": \"sentence\",\n",
    "          \"id\": \"P251867.U2\",\n",
    "          \"label\": \"o 1 - o 3\",\n",
    "          \"cdl\": [\n",
    "            {\n",
    "              \"node\": \"d\",\n",
    "              \"type\": \"line-start\",\n",
    "              \"ref\": \"P251867.2\",\n",
    "              \"n\": \"1\",\n",
    "              \"label\": \"o 1\"\n",
    "            },\n",
    "            {\n",
    "              \"node\": \"l\",\n",
    "              \"frag\": \"{ÅeÅ¡}maâ-durah-abzu\",\n",
    "              \"id\": \"P251867.l1ac01\",\n",
    "              \"ref\": \"P251867.2.1\",\n",
    "              \"inst\": \"Madurahabzu[1]ON\",\n",
    "              \"sig\": \"@dcclt%sux:{ÅeÅ¡}maâ-durah-abzu=Madurahabzu[1//1]ON'ON$Madurahabzu/{ÅeÅ¡}maâ-durah-abzu#~\",\n",
    "              \"f\": {\n",
    "                \"lang\": \"sux\",\n",
    "                \"form\": \"{ÅeÅ¡}maâ-durah-abzu\",\n",
    "                \"gdl\": [\n",
    "                  {\n",
    "                    \"det\": \"semantic\",\n",
    "                    \"pos\": \"pre\",\n",
    "                    \"seq\": [\n",
    "                      {\n",
    "                        \"v\": \"ÅeÅ¡\",\n",
    "                        \"id\": \"P251867.2.1.0\"\n",
    "                      }\n",
    "                    ]\n",
    "                  },\n",
    "```\n",
    "\n",
    "The first `cdl` key contains a list that has a single element, a dictionary (the ending square bracket of this list is not included in the snippet). This dictionary is a `c` node (Chunk) representing the entire text. The `c` node contains a new `cdl` key which has a list of dictionaries as its value including two `d` (Discontinuity) nodes (`object` and `obverse`) and another `c` node that represents a discourse unit, namely the body of the text (note that Chunk `text` and Chunk `body` are identical here - but that need not be the case). Eventually, there is a node `l` that contains the transliteration and lemmatization data for the first word of this text.\n",
    "\n",
    "This hierarchy implies that a word (an \"l\" node) may belong to different hierarchies that do not necessarily align. For instance, a word may belong to a sentence (a Chunk) that continues from the obverse to the reverse (Discontinuities) of a tablet. The JSON structure allows to express (and to retrieve) those facts simultaneously.\n",
    "\n",
    "In order to pull out the lemmatization data we need to iterate through the hierarchy of `cdl` keys until we encounter an `l` node, containing an `f` key. The value of the `f` key is the data we want.\n",
    "\n",
    "A straightforward way of doing this is by defining a recursive function, that is, a function that calls itself to recursively inspect the value of successive layers of `cdl` keys until one encounters an `f` key. The contents of the `f` key are then added to a list. In its most basic form that function looks like this:\n",
    "\n",
    "```python\n",
    "def parsejson(text):\n",
    "    l = []\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        if \"cdl\" in JSONobject: \n",
    "            l.extend(parsejson(JSONobject))\n",
    "        if \"f\" in JSONobject:\n",
    "            l.append(JSONobject[\"f\"])\n",
    "    return l\n",
    "```\n",
    "\n",
    "The function is called with the argument `text`, which contains the contents of the entire JSON file, as retrieved above. The function adds a new row of lemmatization data (one word at a time) each time it encounters an `f` key. Finally it returns the list (a list of dictionaries) with the lemmatization data. The function may be called as follows:\n",
    "\n",
    "```python\n",
    "lemm_l = []\n",
    "lemm_l.extend(parsejson(text))\n",
    "```\n",
    "By including the call to `parsejson()` in a loop, we can fill lemm_l with the lemmatization data of multiple texts.\n",
    "\n",
    "The list `lemm_l` now contains all the lemmatization data of [P230754](http://oracc.org/obmc/P230754) as edited in [OBMC](http://oracc.org/obmc). We can inspect the data by reading it into a `pandas` DataFrame. \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "words = pd.DataFrame(lemm_l).fillna(\"\")\n",
    "words\n",
    "```\n",
    "The function `fillna()` from the `pandas` library fills holes in the DataFrame where no data are available. For instance, a word that has not been lemmatized has no data in the fields \"cf\" (Citation Form), \"gw\" (Guide Word), and \"pos\" (Part of Speech). Without this function empty slots in the DataFrame will have the value NaN (or: \"Not a Number\"), which can be problematic in further data manipulation. The argument of the `fillna()` function is the value to be placed in empty cells - in this case the empty string. Note that NaN and \"\" are of different data types: NaN belongs to a numeric data type; the empty string is a string.\n",
    "\n",
    "One may write the DataFrame directly to a `csv` (or some similar file format), but it is often more useful to structure the data a bit more (section [2.1.7](#2.1.7-Data-Structuring)). Before we get to that we will first discuss several enhancements of the `parsejson()` function.\n",
    "\n",
    "![P251867](http://cdli.ucla.edu/dl/tn_photo/P251867.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3.4 The `parsejson()` function\n",
    "The `parsejson()` function will \"dig into\" the `json` file (transformed into a dictionary) until it finds the relevant data. The `json` file consists of a hierarchy of `cdl` nodes; only the lowest nodes contain lemmatization data. The function goes down this hierarchy by calling itself when another `cdl` node is encountered. For more information about the data hierarchy in the [ORACC](http://oracc.org) `json` files, see [ORACC Open Data](http://oracc.org/doc/opendata/index.html).\n",
    "\n",
    "The first argument of the `parsejson()` function is a `JSON` object, essentially a Python dictionary that initially contains the entire contents of the original JSON file. The code takes the key `cdl` the value of which is a list of `JSON` dictionaries. Iterating through these dictionaries, if a dictionary contains another `cdl` node, the function calls itself with this lower-level dictionary as argument. This way the function digs deeper and deeper into the `JSON` tree, until it does not encounter a `cdl` key anymore. Here we are at the level of individual words. The code checks for a key `f`, if it exists the value of that key (another dictionary) is appended to the list `lemmas`. The list `lemmas` will become a list of dictionaries, where each dictionary represents a single word and the list represents the vocabulary of a single text with the words in their original order.\n",
    "\n",
    "The second argument of the function, `id_text`, consists of a project abbreviation, such as `blms` or `cams/gkab` plus a text ID, in the format `cams/gkab/P338616` or `dcclt/Q000039`. This ID is added to the lemmatization data of every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsejson(text, id_text):\n",
    "    lemmas = []\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        if \"cdl\" in JSONobject: \n",
    "            lemmas.extend(parsejson(JSONobject, id_text))\n",
    "        if \"f\" in JSONobject:\n",
    "            lemm = JSONobject[\"f\"]\n",
    "            lemm[\"id_text\"] = id_text\n",
    "            lemmas.append(lemm)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Call the `parsejson()` function for every `JSON` file\n",
    "The code in this cell will iterate through the list of projects entered above (1.1). For each project the `JSON` zip file is located in the directory `jsonzip`, named PROJECT.zip. The `zip` file contains a directory that is called `corpusjson` that contains a JSON file for every text that is available in that corpus. The files are called after their text IDs in the pattern `P######.json` (or `Q######.json` or `X######.json`).\n",
    "\n",
    "The function `namelist()` of the `zipfile` package is used to create a list of the names of all the files in the ZIP. From this list we select all the file names in the `corpusjson` directory with extension `.json` (this way we exclude the name of the directory itself). \n",
    "\n",
    "Each of these files is read from the `zip` file and loaded with the command `json.loads()`, which transforms the string into a proper JSON object. \n",
    "\n",
    "This JSON object (essentially a Python dictionary), which is called `data_json` is now sent to the `parsejson()` function. The function returns a list of lemmata (each lemma represented by a dictionary). The `extend()` method is used to add this list to the list `lemm_l`, one document at the time. In the end, `lemm_l` will contain as many list elements as there are words in all the texts in the projects requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_l = [] # initiate the list that will hold all the lemmatization data of all texts in all requested projects\n",
    "for project in project_list:\n",
    "    file = f\"jsonzip/{project.replace('/', '-')}.zip\"\n",
    "    try:\n",
    "        zip_file = zipfile.ZipFile(file)       # create a Zipfile object\n",
    "    except:\n",
    "        errors = sys.exc_info() # get error information\n",
    "        print(file), print(errors[0]), print(errors[1]) # and print it\n",
    "        continue\n",
    "    files = zip_file.namelist()     # list of all the files in the ZIP\n",
    "    files = [name for name in files if \"corpusjson\" in name and name[-5:] == '.json']                                                                                                  #that holds all the P, Q, and X numbers.\n",
    "    for filename in tqdm(files, desc=project):                            #iterate over the file names\n",
    "        id_text = project + filename[-13:-5] # id_text is, for instance, blms/P414332\n",
    "        try:\n",
    "            text = zip_file.read(filename).decode('utf-8')         #read and decode the json file of one particular text\n",
    "            data_json = json.loads(text)                # make it into a json object (essentially a dictionary)\n",
    "            lemm_l.extend(parsejson(data_json, id_text))               # and send to the parsejson() function\n",
    "        except:\n",
    "            errors = sys.exc_info() # get error information\n",
    "            print(filename), print(errors[0]), print(errors[1]) # and print it\n",
    "    zip_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Structuring\n",
    "### 3.1 Transform the Data into a DataFrame\n",
    "The list `lemm_l` is transformed into a `pandas` dataframe (`word_df`) for further manipulation. The `pandas` function `fillna('')` fills gaps in the dataframe. If a particular data field is not available for a particular row, `pandas` will fill that cell with NaN (\"Not a Number\"). The data type of NaN is numeric, which creates problems in string manipulation further down the road (for instance in creating a Lemma column, section 3.2). The function `fillna()` will fill all empty cells with the argument it is given - in this case the empty string.\n",
    "\n",
    "For various reasons not all JSON files will have all data types that potentially exist in an [ORACC](http://oracc.org) lemmatization. For instance, only Sumerian words have a `base`, so if your data set has no Sumerian, this column will not exist in the DataFrame. Where such fields are referenced below, the code may fail and you may need to adjust some lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame(lemm_l)\n",
    "word_df = word_df.fillna('')      # replace NaN (Not a Number) with empty string\n",
    "word_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Remove Spaces and Commas from Guide Word and Sense\n",
    "Spaces and commas in Guide Word and Sense may cause trouble in computational methods in tokenization, or when saved in Comma Separated Values format. All spaces and commas are replaced by hyphens and nothing (empty string), respectively. By default the `replace()` function in `pandas` will match the entire string (that is, \"lugal\" matches \"lugal\" but there is no match between \"l\" and \"lugal\"). In order to match partial strings the parameter `regex` must be set to `True`.\n",
    "\n",
    "The `replace()` function takes a nested dictionary as argument. The top-level keys identify the columns on which the `replace()` function should operate (in this case 'gw' and 'sense'). The value of each key is another dictionary with the search string as key and the replace string as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findreplace = {' ' : '-', ',' : ''}\n",
    "word_df = word_df.replace({'gw' : findreplace, 'sense' : findreplace}, regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create a `lemma` column\n",
    "A lemma, [ORACC](http://oracc.org) style, combines Citation Form, GuideWord and POS into a unique reference to one particular lemma in a standard dictionary, as in `lugal[king]N` (Sumerian) or `Å¡arru[king]N` (Akkadian). Usually, not all words in a text are lemmatized, because a word may be (partly) broken and/or unknown. Unlemmatized and unlemmatizable words will receive a place-holder lemmatization that consists of the transliteration of the word (instead of the Citation Form), with `NA` as GuideWord and `NA` as POS, as in `i-bu-x[NA]NA`. Note that `NA` is a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df[\"lemma\"] = word_df[\"cf\"] + '[' + word_df[\"gw\"] + ']' + word_df[\"pos\"]\n",
    "word_df.loc[word_df[\"cf\"] == \"\" , 'lemma'] = word_df['form'] + '[NA]NA'\n",
    "word_df[['id_text', 'lemma']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Group by Textid\n",
    "Get all the lemmas that belong to a single text in one row (one row = one document). The `agg()` (aggregate) function, which works on the result of a `groupby()` process aggregates columns of the original dataframe. The aggregate function takes a dictionary in which the keys are column names and the values are functions to be used in the aggregation process. The example below has only one such function (`' '.join` will join all entries in the colum `lemma` with a space in between); one may specify (the same or different) functions for different columns, for instance:\n",
    "```python\n",
    "word_df = word_df.groupby(\"textid\").agg({\"lemma\": ' '.join, \"base\": ' '.join})\n",
    "```\n",
    "The process will create a compound index. It is useful to flatten the index with a reset (`reset_index()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = word_df.groupby(\"id_text\").agg({\"lemma\": ' '.join})\n",
    "doc_df = doc_df.reset_index()\n",
    "doc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Save the Results\n",
    "## 4.1 Save Results in CSV file\n",
    "The output file is called `parsed.csv` and is placed in the directory `output`. In most cases, `csv` files open automatically in Excel. This program does not deal well with `utf-8` encoding. If you intend to use the file in Excel, change `encoding ='utf-8'` to `encoding='utf-16'`. For usage in computational text analysis applications `utf-8` is usually preferred. The option `index=False` excludes the (numerical) index from the saved `csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "savefile =  'parsed.csv'\n",
    "with open(f'output/{savefile}', 'w', encoding=\"utf-8\") as w:\n",
    "    doc_df.to_csv(w, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Pickle\n",
    "The Pandas function `to_pickle()` writes a binary file that can be opened in a later phase of the project with the `read_pickle()` command and will reproduce exactly the same DataFrame. The resulting file cannot be used in other programs but preserves the data structure of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled = \"output/parsed.p\"\n",
    "doc_df.to_pickle(pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
