{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(2.2)=\n",
    "# 2.2 Data Acquisition ETCSL\n",
    "## 2.2.1. Introduction\n",
    "\n",
    "The Electronic Text Corpus of Sumerian Literature ([ETCSL](http://etcsl.orinst.ox.ac.uk)) provides editions and translations of 394 Sumerian literary texts, mostly from the Old Babylonian period (around 1800 BCE). The project was founded by Jeremy Black (Oxford University), who sadly passed away in 2004; it was active from 1998 to 2006, when it was archived. Information about the project, its stages, products and collaborators may be found in the project's [About](http://etcsl.orinst.ox.ac.uk/edition2/general.php) page. By the time of its inception [ETCSL](http://etcsl.orinst.ox.ac.uk) was a pioneering effort - the first large digital project in Assyriology, using well-structured data according to the standards and best practices of the time. [ETCSL](http://etcsl.orinst.ox.ac.uk) allows for various kinds of searches in Sumerian and in English translation and provides lemmatization for each individual word. Numerous scholars contributed data sets to the [ETCSL](http://etcsl.orinst.ox.ac.uk) project (see [Acknowledgements](http://etcsl.orinst.ox.ac.uk/edition2/credits.php#ack)). The availability of [ETCSL](http://etcsl.orinst.ox.ac.uk) has fundamentally altered the study of Sumerian literature and has made this literature available for undergraduate teaching.\n",
    "\n",
    "```{figure} ../images/JeremyBlack.jpg\n",
    ":figclass: margin\n",
    "[Jeremy Black](https://cdli.ox.ac.uk/wiki/doku.php?id=black_jeremy_allen) 1951-2004\n",
    "```\n",
    "\n",
    "The original [ETCSL](http://etcsl.orinst.ox.ac.uk) files in TEI XML are stored in the [Oxford Text Archive](http://hdl.handle.net/20.500.12024/2518) from where they can be downloaded as a ZIP file under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License ([by-nc-sa 3.0](http://creativecommons.org/licenses/by-nc-sa/3.0/)). The copyright holders are Jeremy Black, Graham Cunningham, Jarle Ebeling, Esther Flückiger-Hawker, Eleanor Robson, Jon Taylor, and Gábor Zólyomi.\n",
    "\n",
    "The [Oxford Text Archive](http://hdl.handle.net/20.500.12024/2518) page offers the following description:\n",
    "\n",
    "> The Electronic Text Corpus of Sumerian Literature (ETCSL) comprises transliterations and English translations of 394 compositions attested on sources dating to the period from approximately 2100 to 1700 BCE. The compositions are divided into seven categories: ancient literary catalogues; narrative compositions; royal praise poetry and hymns to deities on behalf of rulers; literary letters and letter-prayers; divine and temple hymns; proverbs and proverb collections; and a more general category including compositions such as debates, dialogues and riddles. The numbering of the compositions within the corpus follows Miguel Civil's unpublished catalogue of Sumerian literature (etcslfullcat.html). Files with an initial c are composite transliterations (a reconstructed text editorially assembled from the extant exemplars but including substantive variants) in which the cuneiform signs are represented in the Roman alphabet. Files with an initial t are translations. The composite files include full references for the cuneiform sources and author-date references for the secondary sources (detailed in bibliography.xml). The composite and translation files are in XML and have been annotated according to the TEI guidelines. In terms of linguistic information, each word form in the composite transliterations has been assigned to a lexeme which is specified by a citation form, word class information and basic English translation.\n",
    "\n",
    "Since [ETCSL](http://etcsl.orinst.ox.ac.uk) is an archival site, the editions are not updated to reflect new text finds or new insights in the Sumerian language. Many of the [ETCSL](http://etcsl.orinst.ox.ac.uk) editions were based on standard print editions that itself may have been 10 or 20 years old by the time they were digitized. Any computational analysis of the [ETCSL](http://etcsl.orinst.ox.ac.uk) corpus will have to deal with the fact that: \n",
    "\n",
    "- the text may not represent the latest standard\n",
    "- the [ETCSL](http://etcsl.orinst.ox.ac.uk) corpus is extensive - but does not cover all of Sumerian literature known today\n",
    "\n",
    "```{figure} ../images/P346466.jpg\n",
    ":scale: 50%\n",
    "Sumerian literary fragment: [UET 6, 427](http://cdli.ucla.edu/P346466) with a few lines of [Inana's Descent to the Netherworld](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.4.1&amp;amp;amp;display=Crit&amp;amp;amp;charenc=gcirc#)\n",
    "```\n",
    "\n",
    "In terms of data acquisition, one way to deal with these limitations is to make the [ETCSL](http://etcsl.orinst.ox.ac.uk) data as much as possible compatible with the data standards of the Open Richly Annotated Cuneiform Corpus ([ORACC](http://oracc.org)). [ORACC](http://oracc.org) is an active project where new or updated editions can be produced. If compatible, if [ETCSL](http://etcsl.orinst.ox.ac.uk) and [ORACC](http://oracc.org) data may be freely mixed and matched, then the [ETCSL](http://etcsl.orinst.ox.ac.uk) data set can effectively be updated and expanded.\n",
    "\n",
    "The [ETCSL](http://etcsl.orinst.ox.ac.uk) text corpus was one of the core data sets for the development of of [ePSD1](http://psd.museum.upenn.edu/epsd1/index.html) and [ePSD2](http://oracc.org/epsd2), and the ePSD version of the [ETCSL](http://etcsl.orinst.ox.ac.uk) data forms the core of the literary corpus collected in [ePSD2/literary](http://oracc.org/epsd2/literary). In order to harvest the [ETCSL](http://etcsl.orinst.ox.ac.uk) data for [ePSD2](http://oracc.org/epsd2) the lemmatization was adapted to [ORACC](http://oracc.org) standards and thus the [ePSD2/literary](http://oracc.org/epsd2/literary) version of the [ETCSL](http://etcsl.orinst.ox.ac.uk) dataset is fully compatible with any [ORACC](http://oracc.org) dataset, and can be parsed with the ORACC parser, discussed in section [2.1](2.1). However, [ePSD2/literary](http://oracc.org/epsd2/literary) is not identical with the [ETCSL](http://etcsl.orinst.ox.ac.uk) data set. Several compositions have been replaced by more recent editions (for instance the Sumerian disputations edited in the [ORACC](http://oracc.org) project [Datenbank Sumerischer Streitliteratur](http://oracc.org/dsst)); a significant number of texts that were not available in [ETCSL](http://etcsl.orinst.ox.ac.uk) have been added (many of them published after 2006) and the Gudea Cylinders have been moved to [epsd2/royal](http://oracc.org/epsd2/royal/Q000377), where they more properly belong.\n",
    "\n",
    "For some applications, therefore, parsing the original [ETCSL](http://etcsl.orinst.ox.ac.uk) XML TEI files has become redundant since a fuller and more up-to-date version of the data set is available in [epsd2/literary](http://oracc.org/epsd2/literary). . However, any data transformation implies choices and it is hard to know what the needs will be of future computational approaches to the [ETCSL](http://etcsl.orinst.ox.ac.uk) dataset. The reason to include and discuss the [ETCSL](http://etcsl.orinst.ox.ac.uk) parser here is, first, to offer users the opportunity to work with the original data set. The various transformations included in the current parser may be adapted and adjusted to reflect the preferences and research questions of the user. As a concrete example of choices to be made, [ETCSL](http://etcsl.orinst.ox.ac.uk) distinguishes between main text, secondary text, and additional text, to reflect different types of variants between manuscripts (see below [2.2.4](2.2.4)). The [ePSD2/literary](http://oracc.org/epsd2/literary) data set does not include this distinction. The output of the current parser will indicate for each word whether it is \"secondary\" or \"additional\" (according to [ETCSL](http://etcsl.orinst.ox.ac.uk) criteria) and offer the possibility to include such words or exclude them from the analysis. Similarly, the translations are not included in the [ePSD2/literary](http://oracc.org/epsd2/literary) dataset, nor are they considered by the present parser. Translation data are, however, available in the [ETCSL](http://etcsl.orinst.ox.ac.uk) XML TEI file set and the XML of the transcription files marks the beginning and end of translation paragraphs. Such data, therefore, is available and one may well imagine research questions for which the translation files are relevant (e.g. translation alignment). Although the present code does not deal with translation, one may use the same techniques and the same approach exemplified here to retrieve such data.\n",
    "\n",
    "In order to achieve compatibility between [ETCSL](http://etcsl.orinst.ox.ac.uk) and [ORACC](http://oracc.org) the code uses a number of equivalency dictionaries, that enable replacement of characters, words, or names. These equivalency dictionaries are made available in JSON format (for JSON see section [2.1.1](2.1.1) in the file `equivalencies.json` in the directory `equivalencies`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1.1 XML\n",
    "The [ETCSL](http://etcsl.orinst.ox.ac.uk) files as distributed by the [Oxford Text Archive](http://hdl.handle.net/20.500.12024/2518) are encoded in a dialect of XML (Extensible Markup Language) that is referred to as `TEI` (Text Encoding Initiative). In this encoding each word (in transliteration) is an *element* that is surrounded by `<w>` and `</w>` tags. Inside the start-tag the word may receive several attributes, encoded as name/value pairs, as in the following random examples:\n",
    "\n",
    "```xml\n",
    "<w form=\"ti-a\" lemma=\"te\" pos=\"V\" label=\"to approach\">ti-a</w>\n",
    "<w form=\"e2-jar8-bi\" lemma=\"e2-jar8\" pos=\"N\" label=\"wall\">e2-jar8-bi</w>\n",
    "<w form=\"ickila-bi\" lemma=\"ickila\" pos=\"N\" label=\"shell\"><term id=\"c1813.t1\">ickila</term><gloss lang=\"sux\" target=\"c1813.t1\">la</gloss>-bi</w>\n",
    "```\n",
    "\n",
    "The `form` attribute is the full form of the word, including morphology, but omitting flags (such as question marks), indication of breakage, or glosses. The `lemma` attribute is the form minus morphology (similar to `Citation Form` in [ORACC](http://oracc.org). Some lemmas may be spelled in more than one way in Sumerian; the `lemma` attribute will use a standard spelling (note, for instance, that the `lemma` of \"ti-a\" is \"te\"). The `lemma` in [ETCSL](http://etcsl.orinst.ox.ac.uk) (unlike `Citation Form` in [ORACC](http://oracc.org)) uses actual transliteration with hyphens and sign index numbers (as in `lemma = \"e2-jar8\"`, where the corresponding [ORACC](http://oracc.org) `Citation Form` is [egar](http://oracc.org/epsd2/o0026723).\n",
    "\n",
    ":::{note} ETCSL vs ORACC: terminology and conventions\n",
    ":class: tip, dropdown\n",
    "\n",
    "| Data Type  | ETCSL term | ETCSL example  | ORACC term    | ORACC example |\n",
    "| --- | --- | --- | --- | ---- |\n",
    "| Transliteration | form | e2-jar8-bi | form | e₂-gar₈-bi |\n",
    "| Dictionary Form | lemma  | e2-jar8  | citation form | egar | \n",
    "| Part of Speech | pos  | N  | pos | N |\n",
    "| Basic Translation | label | wall | guide word | wall |\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "The `label` attribute gives a general indication of the meaning of the Sumerian word but is not context-sensitive. That is, the `label` of \"lugal\" is always \"king\", even if in context the word means \"owner\". The `pos` attribute gives the Part of Speech, but again the attribute is not context-sensitive. Where a verb (such as sag₉, to be good) is used as an adjective the `pos` is still \"V\" (for verb). Together `lemma`, `label`, and `pos` define a Sumerian lemma (dictionary entry).\n",
    "\n",
    "In parsing the [ETCSL](http://etcsl.orinst.ox.ac.uk) files we will be looking for the `<w>` and `</w>` tags to isolate words and their attributes. Higher level tags identify lines (`<l>` and `</l>`), versions, secondary text (found only in a minority of sources), etcetera. The XML data structure is hierarchical and may be represented as a tree with a trunk, main branches, and ever smaller branches that all, eventually, connect to the same tree.\n",
    "\n",
    "The [ETCSL](http://etcsl.orinst.ox.ac.uk) file set includes the file [etcslmanual.html](http://etcsl.orinst.ox.ac.uk/edition2/etcslmanual.php) with explanations of the tags, their attributes, and their proper usage.\n",
    "\n",
    "Goal of the parsing process is to get as much information as possible out of the XML tree in a format that is as close as possible to the output of the [ORACC](http://oracc.org/) parser. The output of the parser is a word-by-word (or rather lemma-by-lemma) representation of the entire [ETCSL](http://etcsl.orinst.ox.ac.uk) corpus. For most computational projects it will be necessary to group words into lines or compositions, or to separate out a particular group of compositions. The data is structured in such a way that that can be achieved with a standard set of Python functions of the `pandas` library.\n",
    "\n",
    "### 2.2.1.2 Parsing XML: Xpath, and lxml\n",
    "\n",
    ":::{margin}\n",
    "For proper introductions to `Xpath` and `lxml` see the [Wikipedia](https://en.wikipedia.org/wiki/XPath) article on `Xpath` and the homepage of the [lxml](https://lxml.de/) library, respectively.\n",
    ":::\n",
    "\n",
    "There are several Python libraries specifically for parsing XML, among them the popular `ElementTree` and its twin `cElementTree`. The library `lxml` is largely compatible with `ElementTree` and `cElementTree` but differs from those in its full support of `Xpath`. `Xpath` is a language for finding and retrieving elements and attributes in XML trees. `Xpath` is not a program or a library, but a set of specifications that is implemented in a variety of software packages in different programming languages. \n",
    "\n",
    "`Xpath` uses the forward slash to describe a path through the hierarchy of the XML tree. The expression `\"/body/l/w\"` refers to all the `w` (word) elements that are children of `l` (line) elements that are children of the `body` element in the top level of XML hierarchy.\n",
    "\n",
    "The expression `'//w'`means: all the `w` nodes, wherever in the hierarchy of the XML tree. The expression may be used to create a list of all the `w` nodes with all of their associated attributes. The attributes of a node are addressed  with the `@` sign, so that `//w/@label` refers to the `label` attributes of all the `w` nodes at any level in the hierarchy. \n",
    "\n",
    "```python\n",
    "words = tree.xpath('//w')\n",
    "labels = tree.xpath('//w/@label')\n",
    "```\n",
    "\n",
    "Predicates are put between square brackets and describe conditions for filtering a node set. The expression  `//w[@emesal]` will return all the `w` elements that have an attribute `emesal`. \n",
    "\n",
    "`Xpath` also defines hundreds of functions. An important function is `'string()'` which will return the string value of a node or an attribute.  Once all `w` nodes are listed in the list `words` (with the code above) one may extract the transliteration and Guide Word (`label` in [ETCSL](http://etcsl.orinst.ox.ac.uk)) of each word as follows:\n",
    "\n",
    "```python\n",
    "form_l = []\n",
    "gw_l = []\n",
    "for node in words:\n",
    "    form = node.xpath('string(.)') \n",
    "    form_l.append(form)\n",
    "    gw = node.xpath('string(@label)')\n",
    "    gw_l.append(gw)\n",
    "```\n",
    "\n",
    "The dot, the argument to the `string()` function in `node.xpath('string(.)')`, refers to the current node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1.3 Parsing the XML Tree\n",
    "\n",
    "The module `etree` from the `lxml` library is used to parse the XML files. The code basically works from the highest level of the hierarchy of the XML tree to the lowest, in the following way:\n",
    "\n",
    "```\n",
    "corpus\t\t\t\t\t\t\t\t\tmain process\n",
    "\ttext\t\t\t\t\t\t\t\tparsetext()\n",
    "\t\tversion\t\t\t\t\t\t\tgetversion()\n",
    "\t\t\tsegment\t\t\t\t\t\tgetsegment()\n",
    "\t\t\t\tline\t\t\t\t\tgetline()\n",
    "\t\t\t\t\tword\t\t\t\tgetword()\n",
    "\t\t\t\t\t\tformat\t\t\tetcsl_to_oracc()\n",
    "```\n",
    "\n",
    "Each of these functions divides the XML tree into smaller parts (versions, segments, lines, words) and sends one such smaller part of the tree to the next function. The functions do not return anything. Instead, they modify the dictionary `meta_d` by adding or changing keys that hold meta-data such as version name, line number, etc. Once arrived at the word level (the most basic level of this tree) the dictionary `meta_d` will hold accurate information about the composition name, the version name, the line number, etc. for this particular word. The function `getword()` will gather lemmatization information (form, citation form, part of speech, etc.) from the attributes of the `w` node, and meta-data from `meta_d`.\n",
    "\n",
    "The function `etcsl_to_oracc()`, the last one in the hierarchy, transforms the [ETCSL](https://etcsl.orinst.ox.ac.uk/) style lemma into a [ORACC](http://oracc.org/) style lemma and appends the resulting dictionary data to a list (a list of dictionaries). In the end, each word in the entire [ETCSL](https://etcsl.orinst.ox.ac.uk/) corpus will have its own entry in this list.\n",
    "\n",
    "The word `šeŋ₆-ŋa₂` in the file `c.1.2.2.xml` ([Enlil and Sud](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.2.2&display=Crit&charenc=gcirc#)), in Version A, segment A line 115, looks as follows in the original XML file: \n",
    "\n",
    "```xml\n",
    "<w form=\"cej6-ja2\" lemma=\"cej6\" pos=\"V\" label=\"to be hot\">cej6-ja2</w>\n",
    "```\n",
    "\n",
    "The dictionary that is the result of the parsing process represents that same word as follows:\n",
    "\n",
    "```python\n",
    "{'id_text': 'c.1.2.2', \n",
    " 'text_name': 'Enlil and Sud',\n",
    " 'version': 'A', \n",
    " 'lang': 'sux',\n",
    " 'cf': 'šeŋ',\n",
    " 'gw': 'cook',\n",
    " 'pos': 'V/t',\n",
    " 'form': 'šeŋ₆-ŋa₂',\n",
    " 'label' : 'A115',\n",
    " 'id_line': 109,\n",
    " 'extent': ''}\n",
    "```\n",
    "\n",
    ":::{note}\n",
    "\n",
    "In the process the transliteration and lemmatization data have been replaced by [ePSD2](http://oracc.org/epsd2) style data and terminology: ('gw' :  'cook' instead of 'label= \"to be hot\"' and 'cf' : 'šeŋ' instead of 'lemma=\"cej6\"').  \n",
    "\n",
    ":::\n",
    "\n",
    "The sections below will discuss in some detail the various functions, starting with the pre-processing functions and going up the hierarchy from `etcsl_to_oracc()` to  `parsetext()`. \n",
    "\n",
    ":::{note}\n",
    "\n",
    "The logic of the entire process goes from `parsetext()` via `getversion()` etc. down to `etcsl_to _oracc()`. Each of these functions calls the next one and since a function cannot be called before it is defined, we have to define the last function first and deal with them in the opposite order.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Setting Up\n",
    "### 2.2.2.1 Import Packages\n",
    "\n",
    ":::{margin}\n",
    "For proper installation of packages for a Jupyter Notebook see [1.4.3 install_packages](1.4.3).\n",
    ":::\n",
    "\n",
    "Before running this cell you may need to install the package `lxml` (for parsing XML) by running \n",
    "```python\n",
    "%conda install lxml\n",
    "```\n",
    "\n",
    ":::{important}\n",
    "\n",
    "This notebook expects the following files and directories:\n",
    "\n",
    "1. Directory `etcsl/transliterations/`  \n",
    "   This directory should contain the [ETCSL](http://etcsl.orinst.ox.ac.uk) `TEI XML` transliteration files. The files may be downloaded from the [Oxford Text Archive](http://hdl.handle.net/20.500.12024/2518). The files are found in the directory `transliterations` in the file `etcsl.zip`, .\n",
    "2. Directory `Equivalencies`  \n",
    "   `equivalencies.json`: a set of equivalency dictionaries used at various places in the parser. The directory and the file are included in the [COMPASS](https://github.com/niekveldhuis/compass) file set (see section [1.4.1](1.4.1).\n",
    "3. Directory `output`.\n",
    "The output is saved in the `output` directory as a single `.csv` file. If the directory does not exist, it is created in the next cell.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* re: regular expressions\n",
    "* lxml: tools for parsing an XML tree\n",
    "* os: for basic Operating System operations (such as creating a directory)\n",
    "* json: for reading the equivalency dictionaries in JSON format\n",
    "* pandas: data analysis and manipulation; dataframes\n",
    "* tqdm: progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from lxml import etree\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "os.makedirs('output', exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2.2.2.2)=\n",
    "### 2.2.2.2 Load Equivalencies \n",
    "The file `equivalencies.json` contains a number of dictionaries that will be used to search and replace at various places in this notebook. The dictionaries are:\n",
    "- `suxwords`: Sumerian words (Citation Form, GuideWord, and Part of Speech) in [ETCSL](http://etcsl.orinst.ox.ac.uk) format and their [ORACC](http://oracc.org) counterparts.\n",
    "- `emesalwords`: idem for Emesal words\n",
    "- `propernouns`: idem for proper nouns\n",
    "- `ampersands`: HTML entities (such as `&aacute;`) and their Unicode counterparts (`á`; see section [2.2.3](2.2.3)).\n",
    "- `versions`: [ETCSL](http://etcsl.orinst.ox.ac.uk) version names and (abbreviated) equivalents\n",
    "\n",
    "The `equivalencies.json` file is loaded with the `json` library (for JSON and the `json` library, see section [2.1.1](2.1.1)). The dictionaries `suxwords`, `emesalwords` and `propernouns` (which, together, contain the entire [ETCSL](http://etcsl.orinst.ox.ac.uk) vocabulary) are concatenated into a single dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"equivalencies/equivalencies.json\", encoding=\"utf-8\") as f:\n",
    "    eq = json.load(f)\n",
    "equiv = eq[\"suxwords\"]\n",
    "equiv.update(eq[\"emesalwords\"])\n",
    "equiv.update(eq[\"propernouns\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2.2.3)=\n",
    "## 2.2.3 Pre-processing: HTML-entities\n",
    "Before the XML files can be parsed, it is necessary to remove character sequences that are not allowed in XML proper (so-called HTML entities). \n",
    "\n",
    "In non-transliteration contexts (bibliographies, composition titles, etc.) [ETCSL](https://etcsl.orinst.ox.ac.uk/) uses so-called HTML entities to represent non-ASCII characters such as,  á, ü, or š. These entities are encoded with an opening ampersand (`&`) and a closing semicolon (`;`). For instance, `&C;` represents the character `Š`. The HTML entities are for the most part project-specific and are declared in the file `etcsl-sux.ent` which is part of the file package and is used by the [ETCSL](https://etcsl.orinst.ox.ac.uk/) project in the process of validating and parsing the XML for on-line publication.\n",
    "\n",
    "For purposes of data acquisition these entities need to be resolved, because XML parsers will not recognize these sequences as valid XML. \n",
    "\n",
    "The key `ampersands` in the file `equivalencies.json` has as its value a dictionary, listing all the HTML entities that appear in the [ETCSL](https://etcsl.orinst.ox.ac.uk/) files with their Unicode counterparts:\n",
    "\n",
    "```json\n",
    "{ \n",
    " \"&C;\": \"Š\",\n",
    " \"&Ccedil;\": \"Ç\",\n",
    " \"&Eacute;\": \"É\",\n",
    " \"&G;\": \"Ŋ\",\n",
    " \"&H;\": \"H\",\n",
    " \"&Imacr;\": \"Î\",\n",
    " \"&X;\" : \"X\",\n",
    " \"&aacute;\": \"á\"\n",
    "}\n",
    "```\n",
    "\n",
    "etc.\n",
    "\n",
    "This dictionary is used to replace each HTML entity with its unicode (UTF-8) counterpart in the entire corpus.\n",
    "\n",
    ":::{margin}\n",
    "The regular expression `[^;]+` means: a sequence of one or more (`+`) characters, except the semicolon. The symbol `^` is the negation symbol in regular expressions. The expression `&[^;]+;` therefore captures a sequence of any length that begins with an ampersand and ends with a semicolon. There are many introductions for regular expressions on the web, for instance [regular-expressions.info](https://www.regular-expressions.info/), or [An Introduction to Regular Expressions](https://www.oreilly.com/content/an-introduction-to-regular-expressions/) by Thomas Nield.\n",
    ":::\n",
    "\n",
    "The function `ampersands()` is called in `parsetext()` (see section [2.2.11](2.2.11) before the `etree` is built. The function uses the `sub()` function from the `re` (Regular Expressions) module. The arguments of this function are `sub(find_what, replace_with, text)`. In this case, the `find_what` is the compiled regular expression `amp`, matching all character sequences that begin with & and end with a semicolon (;). This regular expression is defined in the main process (see section [2.2.12](2.2.12) as follows:\n",
    "\n",
    "```python\n",
    "amp = re.compile(r'&[^;]+;')\n",
    "```\n",
    "\n",
    "The `replace_with` argument is a temporary `lambda` function that uses the `ampersands` dictionary to find the utf-8 counterpart of the HTML entity. The dictionary is queried with the `get()` function (m.group(0) represents the match of the regular expression `amp`). The `get()` function allows a fall-back argument, to be returned in case the dictionary does not have the key that was requested. This second argument is the actual regular expression match, so that in those cases where the dictionary does not contain the match it is replaced by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ampersands(string):    \n",
    "    string = re.sub(amp, lambda m: \n",
    "               eq[\"ampersands\"].get(m.group(0), m.group(0)), string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2.2.4)=\n",
    "## 2.2.4 Pre-Processing: Additional Text and Secondary Text\n",
    "\n",
    "In order to be able to preserve the [ETCSL](http://etcsl.orinst.ox.ac.uk) distinctions between main text (the default), secondary text, and additional text, such information needs to be added as an attribute to each `w` node (word node). This must take place in pre-processing, before the XML file is parsed.\n",
    "\n",
    "[ETCSL](http://etcsl.orinst.ox.ac.uk) transliterations represent composite texts, put together (in most cases) from multiple exemplars. The editions include substantive variants, which are marked either as \"additional\" or as \"secondary\". Additional text consists of words or lines that are *added* to the text in a minority of sources. In the opening passage of [Inana's Descent to the Netherworld](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.4.1&amp;amp;amp;display=Crit&amp;amp;amp;charenc=gcirc#), for instance, there is a list of temples that Inana leaves behind. One exemplar extends this list with eight more temples; in the composite text these lines are marked as \"additional\" and numbered lines 13A-13H. Secondary text, on the other hand, is variant text (words or lines) that are found in a minority of sources *instead of* the primary text. An example in [Inana's Descent to the Netherworld](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.4.1&amp;amp;amp;display=Crit&amp;amp;amp;charenc=gcirc#) is lines 30-31, which are replaced by 30A-31A in one manuscript (text and translation from [ETCSL](http://etcsl.orinst.ox.ac.uk)):\n",
    "\n",
    "| line | text                                       | translation                                                  |\n",
    "| ---- | ------------------------------------------ | ------------------------------------------------------------ |\n",
    "| 30   | sukkal e-ne-eĝ₃ sag₉-sag₉-ga-ĝu₁₀          | my minister who speaks fair words,                           |\n",
    "| 31   | ra-gaba e-ne-eĝ₃ ge-en-gen₆-na-ĝu₁₀        | my escort who speaks trustworthy words                       |\n",
    "| 30A  | \\[na\\] ga-e-de₅ na de₅-ĝu₁₀ ḫe₂-\\[dab₅\\]    | I am going to give you instructions: my instructions must be followed; |\n",
    "| 31A  | \\[inim\\] ga-ra-ab-dug₄ ĝizzal \\[ḫe₂-em-ši-ak\\] | I am going to say something to you: it must be observed      |\n",
    "\n",
    "\"Secondary text\" and \"additional text\" can also consist of a single word and there are even cases of \"additional text\" within \"additional text\" (an additional word within an additional line).\n",
    "\n",
    "In [ETCSL](http://etcsl.orinst.ox.ac.uk) TEI XML secondary/additional text is introduced by a tag of the type:\n",
    "\n",
    "```xml\n",
    "<addSpan to=\"c141.v11\" type=\"secondary\"/>\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```xml\n",
    "<addSpan to=\"c141.v11\" type=\"additional\"/>\n",
    "```\n",
    "\n",
    "The number c141 represents the text number in [ETCSL](http://etcsl.orinst.ox.ac.uk) (in this case [Inana's Descent to the Netherworld](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.4.1&amp;amp;amp;display=Crit&amp;amp;amp;charenc=gcirc#), text c.1.4.1). The return to the primary text is indicated by a tag of the type:\n",
    "\n",
    "```xml\n",
    "<anchor id=\"c141.v11\"/>\n",
    "```\n",
    "\n",
    "Note that the `id` attribute in the `anchor` tag is identical to the `to` attribute in the `addSpan` tag.\n",
    "\n",
    "We can collect all the `w` tags (words) between `addSpan` and its corresponding `anchor` tag with the following `xpath` expression:\n",
    "\n",
    "```python\n",
    "secondary = tree.xpath('//w[preceding::addSpan[@type=\"secondary\"]/@to = following::anchor/@id]')\n",
    "```\n",
    "\n",
    "In the expression `preceding` and `following` are so-called `axes` (plural of `axis`) which describe the relationship of an element to another element in the tree. The expression means: get all `w` tags that are preceded by an `addSpan` tag and followed by an `anchor` tag. The `addSpan` tag has to have an attribute `type` with value `secondary` , and the value of the `to` attribute of this `addSpan` tag is to be equal to the `id` attribute of the following `anchor` tag.\n",
    "\n",
    "Once we have collected all the \"secondary\" `w` tags this way, we can add a new attribute to each of these words in the following way:\n",
    "\n",
    "```python\n",
    "for word in secondary:\n",
    "    word.attrib[\"status\"] = \"secondary\"\n",
    "```\n",
    "\n",
    "In the process of parsing we can retrieve this new `status` attribute to mark all of these words as `secondary`.\n",
    "\n",
    "Since we can do exactly the same for \"additional text\" we can slightly adapt the above expression for use in the function `mark_extra()`\n",
    "\n",
    "The function `mark_extra()` is called twice by the function `parsetext()` (see below, section [2.2.11](2.2.11)), once for \"additional\" and once for \"secondary\" text, indicated by the `which` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_extra(tree, which):\n",
    "    extra = tree.xpath(f'//w[preceding::addSpan[@type=\"{which}\"]/@to = following::anchor/@id]')\n",
    "    for word in extra:\n",
    "        word.attrib[\"status\"] = which\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2.2.5)=\n",
    "## 2.2.5 Transliteration Conventions\n",
    "\n",
    "The function `tounicode()` is called in the function `getword()` to format Citation Forms and Forms (transliteration). It changes 'c' into  'š', 'j' into 'ŋ', 'e2' into 'e₂', etc. This is done in two steps. First sign index numbers are changed from regular numbers into Unicode index numbers (du3 > du₃). The replacement of sign index numbers is complicated by the fact that Citation Form and Form may include real numbers, as in 7-ta-am3 (\"seven each\") where the 7 should remain unchanged, while am3 should become am₃. The replacement routine for numbers uses a regular expression to search for a letter immediately followed by one or two digits. The regular expression and the accompanying translation table `transind` are defined in the main process (see [2.2.12](2.2.12)), as follows:\n",
    "\n",
    "```python\n",
    "ind = re.compile(r'[a-zŋḫṣšṭA-ZŊḪṢŠṬ][0-9x]{1,2}')\n",
    "ascind, uniind = '0123456789x', '₀₁₂₃₄₅₆₇₈₉ₓ'\n",
    "transind = str.maketrans(ascind, uniind)\n",
    "```\n",
    "\n",
    ":::{margin}\n",
    "\n",
    "In this case it is not necessary to define a so-called look-behind in the regular expression `ind`. The match of `ind` includes the letter that precedes the index number, but since the translation table only has digits, this letter is returned unchanged.\n",
    "\n",
    ":::\n",
    "\n",
    "The replacement code uses the `re.sub()` function with a temporary lambda function to translate the regular digits into their Unicode subscript counterparts. Using a function in `re.sub()` was discussed above in section [2.2.3](2.2.3):\n",
    "\n",
    "Finally,  `tounicode()` uses another `translate()` call to replace 'c' by 'š', 'j' by 'ŋ', etc, using the table `transcj` which is also created in the main process.\n",
    "\n",
    "```python\n",
    "asccj, unicj = 'cjCJ', 'šŋŠŊ'\n",
    "transcj = str.maketrans(asccj, unicj)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tounicode(string):\n",
    "    string = re.sub(ind, lambda m: m.group().translate(transind), string)\n",
    "    string = string.translate(transcj)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2.2.6)=\n",
    "## 2.2.6 Replace ETCSL-style by ORACC-style Lemmatization\n",
    "\n",
    "The function `etcsl_to_oracc()` is called in the function `getword()` with a single argument: the dictionary `word`. This dictionary contains, besides meta data, the citation form, basic meaning ('label') and Part of Speech of a single word in ETCSL-style. The function will look up each lemma (a combination of Citation Form, Guide Word, and Part of Speech) in the dictionary `equiv`. This dictionary is a combination of three dictionaries in the file `equivalencies.json`, namely `suxwords`, `emesalwords` and `propernouns` (see [2.2.2.2](2.2.2.2)). If the lemma is found in `equiv`, the [ETCSL](https://etcsl.orinst.ox.ac.uk/) 'cf', 'gw', and 'pos' are replaced by their [ORACC](http://oracc.org/) counterparts.\n",
    "In the `equiv` dictionary the lemmas are stored in the following format:\n",
    "```json\n",
    "{\"taka₄[to leave behind]V\": {\n",
    "            \"gw\": \"abandon\",\n",
    "            \"pos\": \"V/t\",\n",
    "            \"cf\": \"taka\"\n",
    "        },\n",
    " \"me-te-ŋal₂[seemly]AJ\": {\n",
    "            \"gw\": \"seemly\",\n",
    "            \"pos\": \"AJ\",\n",
    "            \"cf\": \"meteŋal\"\n",
    "        }\n",
    "}\n",
    "```\n",
    "The keys in this dictionary are combinations of 'cf', 'gw', and 'pos' ([ETCSL](https://etcsl.orinst.ox.ac.uk/)-style) in a single string. The `etcsl_to_oracc()` function, therefore first has to create the `lemma` from the fields `cf`, `gw`, and `pos` before it can look the word up in `equiv`. \n",
    "\n",
    "In a few cases a single word in [ETCSL](https://etcsl.orinst.ox.ac.uk/) is represented by a sequence of two words in [ePSD2](http://oracc.org/epsd2) style. This is represented as follows in the `equiv` dictionary:\n",
    "\n",
    "```json\n",
    " {\"maš₂-sa-la₂[bug-ridden goat]N\": {\n",
    "     \t\t\"cf\": \"maš\",\n",
    "     \t\t\"pos\": \"N\",\n",
    "     \t\t\"gw\": \"goat\",\n",
    "     \t\t\"cf2\": \"sala\",\n",
    "     \t\t\"pos2\": \"AJ\",\n",
    "     \t\t\"gw2\": \"bug-ridden\" \n",
    "        },}\n",
    "```\n",
    "\n",
    "The code checks for the existence of a `cf2` key. If present, a new dictionary is created (`word2`) and both dictionaries (`word` and `word2`) are appended to the list `alltexts`.\n",
    "\n",
    "If the lemma is not found in the `equiv` list the `word` dictionary is left unchanged and appended to the list `alltexts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etcsl_to_oracc(word):\n",
    "    lemma = f\"{word['cf']}[{word['gw']}]{word['pos']}\"\n",
    "    if lemma in equiv:\n",
    "        word['cf'] = equiv[lemma][\"cf\"]\n",
    "        word[\"gw\"] = equiv[lemma][\"gw\"]\n",
    "        word[\"pos\"] = equiv[lemma][\"pos\"]\n",
    "        alltexts.append(word)\n",
    "        if \"cf2\" in equiv[lemma]: # if an ETCSL word is replaced by two ORACC words\n",
    "            word2 = word.copy()\n",
    "            word2[\"cf\"] = equiv[lemma][\"cf2\"]\n",
    "            word2[\"gw\"] = equiv[lemma][\"gw2\"]\n",
    "            word2[\"pos\"] = equiv[lemma][\"pos2\"]\n",
    "            alltexts.append(word2)\n",
    "    else: # word not found in equiv\n",
    "        alltexts.append(word)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.7 Formatting Words\n",
    "\n",
    "The function `getword()`, which is called by the function `getline()`,  is the most complex of the series of functions that parses the [ETCSL](http://etcsl.orinst.ox.ac.uk) data. This is the case because different types of words (regular Sumerian words, proper nouns, Emesal words, Akkadian glosses) are processed in different ways.\n",
    "\n",
    "As a first step `getword()` will copy all the meta-data from the dictionary `meta_d` into the dictionary `word`. This dictionary will hold all the data for one individual word token.\n",
    "\n",
    ":::{margin}\n",
    "\n",
    "For Akkadian glosses in Sumerian literary texts, see Szilvia Sövegjártó, [*Sumerische Glossenhandschriften als Quellen des altbabylonischen hermeneutischen Denkens*](https://www.zaphon.de/sumerische-glossenhandschriften/en) (2020).\n",
    "\n",
    ":::\n",
    "\n",
    "If `getword()` receives a `gloss` node, this node represents either an Akkadian word, or an entire Akkadian phrase or sentence, that is inserted in a Sumerian text as a translation gloss. Akkadian words are not lemmatized in [ETCSL](https://etcsl.orinst.ox.ac.uk/), so all we can collect is the `form` (the transliteration) and the language. These fields are added to the `word` dictionary, `word` is appended to the list `alltexts` and control is returned to the previous function (`getline()`), which will send the next word.\n",
    "\n",
    "If `getword()` receives a `w` node (a word) it will assign different attributes of that node to different fields in the `word` dictionary. The Citation Form ('cf') is found in the attribute `lemma`; the Guide Word ('gw') is found in the attribute `label`; and the Part of Speech ('pos') in the attribute `pos`.\n",
    "\n",
    ":::{admonition} Sumerian word in ETCSL XML\n",
    ":class: tip, dropdown\n",
    "```XML\n",
    "<w form=\"i-im-jen\" lemma=\"jen\" pos=\"V\" label=\"to go\">i-im-jen</w>\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "The rest of the code takes care of some special situations:\n",
    "\n",
    "* **Unlemmatized**: If there is no attribute `pos`, this indicates that the word was not lemmatized (because it is broken or unknown). In such cases `pos` and `gw` are both assigned 'NA'. Note that 'NA' is a string, not Missing Value.\n",
    "\n",
    "* **Emesal words** in [ETCSL](http://etcsl.orinst.ox.ac.uk) use their Sumerian equivalents as `citation form` (attribute `lemma`), adding a separate attribute (`emesal`) for the Emesal form proper. This Emesal form is the one that is used as `citation form` in the output.\n",
    "\n",
    ":::{admonition} Emesal word in ETCSL XML\n",
    ":class: tip, dropdown\n",
    "```xml\n",
    "<w form=\"e-ne-ej3\" lemma=\"inim\" pos=\"N\" label=\"word\" emesal=\"e-ne-ej3\">e-ne-ej3</w> \n",
    "```\n",
    "\n",
    "::: \n",
    "\n",
    "* **Proper Nouns**: in [ETCSL](https://etcsl.orinst.ox.ac.uk/) proper nouns are nouns (`pos` = \"N\"), which are qualified by an additional attribute `type` (Divine Name, Personal Name, Geographical Name, etc.; abbreviated as DN, PN, GN, etc.). In [ORACC](http://oracc.org/) a word has a single `pos`; for proper nouns this is DN, PN, GN, etc. - so what is `type` in [ETCSL](https://etcsl.orinst.ox.ac.uk/) becomes `pos` in [ORACC](http://oracc.org/). [ORACC](http://oracc.org/) proper nouns usually do not have a guide word (only a number to enable disambiguation of namesakes). The [ETCSL](https://etcsl.orinst.ox.ac.uk/) guide words (`label`) for names come pretty close to [ORACC](http://oracc.org/) citation forms. Proper nouns are therefore formatted differently from other nouns.\n",
    "\n",
    ":::{admonition} Proper Noun in ETCSL XML\n",
    ":class: tip, dropdown\n",
    "The temple name Eana:\n",
    "\n",
    "```xml\n",
    "<w form=\"e2-an-na-ju10\" lemma=\"e2-an-na\" pos=\"N\" type=\"TN\" label=\"E-ana\">e2-an-na-ju10</w>\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "* **Additional/Secondary**: finally, in pre-processing we added to some `w` nodes an attribute `status`, which is either 'additional' or 'secondary' ([2.2.4](2.2.4)). If the attribute exists, it is added to the `word` dictionary.\n",
    "\n",
    "The dictionary `word` now has all the information it needs, but Citation Form, Guide Word, and Part of Speech are still mostly in [ETCSL](https://etcsl.orinst.ox.ac.uk/) format. The function `getword()` calls `tounicode()` to change (Sumerian) text from ASCII to Unicode representation (see section [2.2.5](2.2.5)). The argument of `tounicode()` is a string, the `form` or the `cf` (Citation Form) of the word that is being processed. The function returns the same string in Unicode representation.\n",
    "\n",
    "The function `getword()`, finally sends the `word` dictionary to `etcsl_to_oracc()` (section [2.2.6](2.2.6)) for final formatting of these data elements and to add the the data to the list `alltexts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getword(node, meta_d):\n",
    "    word = {key:meta_d[key] for key in meta_d} # copy all meta data from meta_d into the word dictionary\n",
    "    \n",
    "    if node.tag == 'gloss': # these are Akkadian glosses which are not lemmatized\n",
    "        form = node.xpath('string(.)')\n",
    "        form = form.replace('\\n', ' ').strip() # occasionally an Akkadian gloss may consist of multiple lines\n",
    "        word[\"form\"] = tounicode(form)\n",
    "        word[\"lang\"] = node.xpath(\"string(@lang)\")\n",
    "        alltexts.append(word)\n",
    "        return\n",
    "    \n",
    "    word[\"cf\"] = node.xpath('string(@lemma)') # xpath('@lemma') returns a list. The string\n",
    "    word[\"cf\"] = word[\"cf\"].replace('Xbr', '(X)')  # function turns it into a single string\n",
    "    word[\"gw\"] = node.xpath('string(@label)')\n",
    "\n",
    "    if node.xpath('@pos'):\n",
    "        word[\"pos\"] = node.xpath('string(@pos)')\n",
    "    else:         # if a word is not lemmatized (because it is broken or unknown) add pos = NA and gw = NA\n",
    "        word[\"pos\"] = 'NA'\n",
    "        word[\"gw\"] = 'NA'\n",
    "\n",
    "    form = node.xpath('string(@form)')\n",
    "    word[\"form\"] = form.replace('Xbr', '(X)')\n",
    "    \n",
    "    if node.xpath('@emesal'):\n",
    "        word[\"cf\"] = node.xpath('string(@emesal)')\n",
    "        word[\"lang\"] = \"sux-x-emesal\"\n",
    "    else:\n",
    "        word[\"lang\"] = \"sux\"\n",
    "\n",
    "    exception = [\"unclear\", \"Mountain-of-cedar-felling\", \"Six-headed Wild Ram\", \n",
    "                     \"The-enemy-cannot-escape\", \"Field constellation\", \n",
    "                     \"White Substance\", \"Chariot constellation\", \n",
    "                 \"Crushes-a-myriad\", \"Copper\"]\n",
    "    \n",
    "    if node.xpath('@type') and word[\"pos\"] == 'N': # special case: Proper Nouns\n",
    "        if node.xpath('string(@type)') != 'ideophone':  # special case in the special case: skip ideophones\n",
    "            word[\"pos\"] = node.xpath('string(@type)')\n",
    "            word[\"gw\"] = '1'\n",
    "            if node.xpath('string(@label)') not in exception:\n",
    "                word[\"cf\"] = node.xpath('string(@label)')\n",
    "                \n",
    "    if node.xpath('@status'):\n",
    "        word['status'] = node.xpath('string(@status)')\n",
    "    \n",
    "    word[\"cf\"] = tounicode(word[\"cf\"])\n",
    "    word[\"form\"] = tounicode(word[\"form\"])\n",
    "    etcsl_to_oracc(word)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.8 Formatting Lines\n",
    "\n",
    "The function `getline()` is called by `getsegment()`. It first updates the field `id_line` in `meta_d`, increasing it by 1. The data type of `id_line` is integer - it is used to keep lines and gaps in correct order.\n",
    "\n",
    "A line may either be an actual line (in Sumerian and/or Akkadian) or a gap (a portion of text lost). Both receive a line reference (`id_line`).\n",
    "\n",
    "If `getline()` receives an `l` node (a line) it will collect `w` nodes (words) and `gloss` nodes with the language attribute `akk`. The `Xpath` expression looks as follows: './/w|.//gloss\\[@lang=\"akk\"\\]'. The \"pipe\" symbol (|) is a logical \"or\" - the condition looks for either a `w` tag or a `gloss` tag with a `lang` attribute that equals \"akk\". This will find Sumerian words and Akkadian glosses.\n",
    "\n",
    "The function iterates over the list of words, sending each word to `getword()`.\n",
    "\n",
    "A gap of one or more lines in the composite text, due to damage to the original cuneiform tablet, is encoded as follows:\n",
    "\n",
    "```xml\n",
    "<gap extent=\"8 lines missing\"/>\n",
    "```\n",
    "If getline() receives a gap node it copies all the meta data in the dictionary `meta_d` into the dictionary `line` and adds a field `extent` (the length of the gap). This data is found in the attribute `extent` of the gap node. This dictionary is then appended to the list `alltexts` and control is returned to the function `getsegment()`. A row in `alltexts`, therefore, usually represents a word, but may also represent a textual gap.\n",
    "\n",
    ":::{admonition} Dealing with gaps: ORACC vs ETCSL\n",
    ":class: tip, dropdown\n",
    "\n",
    "In [ORACC](http://oracc.org), gaps are described with the fields `extent` (a number, or 'n' for unknown),  and `scope` (line, column, obverse, etc.). [ORACC](http://oracc.org) uses a restricted vocabulary for these fields, but [ETCSL](https://etcsl.orinst.ox.ac.uk/) does not. The code currently does not try to make the [ETCSL](https://etcsl.orinst.ox.ac.uk/) encoding of gaps compatible with the [ORACC](http://oracc.org) encoding.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getline(lnode, meta_d):\n",
    "    meta_d[\"id_line\"] += 1\n",
    "    if lnode.tag == 'gap':\n",
    "        line = {key:meta_d[key] for key in [\"id_text\", \"text_name\", \"version\", \"id_line\"]}\n",
    "        line[\"extent\"] = lnode.xpath(\"string(@extent)\")\n",
    "        alltexts.append(line)\n",
    "        return\n",
    "    \n",
    "    for node in lnode.xpath('.//w|.//gloss[@lang=\"akk\"]'):\n",
    "                        # get <w> nodes and <gloss> nodes, but only Akkadian glosses\n",
    "        getword(node, meta_d)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.9 Segments\n",
    "\n",
    "Some compositions in [ETCSL](http://etcsl.orinst.ox.ac.uk) are divided into segments. That is the case, in particular, when a composition has gaps of unknown length. Segment B supposedly follows segment A, but how much text is missing between them cannot be reconstructed. This is the case, for instance, in [The Death of Gilgameš](https://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.8.1.3&display=Crit&charenc=gcirc#), which is rather fragmentarily preserved.\n",
    "\n",
    "```{figure} ../images/P264388.jpg\n",
    ":scale: 25%\n",
    "[UM 29-16-086](http://cdli.ucla.edu/P264388): fragments of the [Death of Gilgameš: Nippur version](https://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.8.1.3&display=Crit&charenc=gcirc#).\n",
    "```\n",
    "\n",
    "The function `getsegment()` is called by `getversion()` and receives the arguments `tree` (the `etree` object representing one version of the composition) and `meta_d` (the dictionary of meta data). The function `getsegment()` first checks to see whether a sub-division into segments is present. \n",
    "\n",
    "Segments are indicated in the XML with a node `div1` with the attribute `type=\"segment\"`. Segment names (usually a capital letter) are found in an attribute of `div1` called `n`.  \n",
    "\n",
    ":::{admonition} ETCSL XML: version, segment, gap\n",
    ":class: tip, dropdown\n",
    "\n",
    "From the beginning of [The Death of Gilgameš](https://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.8.1.3&display=Crit&charenc=gcirc#). The text is known in multiple versions (from Nippur, from Meturan), and the versions themselves are subdivided into segments, with unknown numbers of lines missing in between.\n",
    "\n",
    "```xml\n",
    "<body>\n",
    "<head lang=\"eng\">A version from Nibru</head>\n",
    "<div1 type=\"segment\" n=\"A\">\n",
    "<gap extent=\"unknown no. of lines missing\"/>\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "The function will collect all `l` (line) *and* `gap` nodes that belong to a single segment. The Xpath expression that is used for that is `.//l|.//gap` (where \"|\" is the \"or\" operator). In some regards gaps are treated as lines - they need to be placed after the last extant line and before the first line after the break. Iterating over this list, if the node is an `l` node the `meta_d` dictionary is updated with a (human-legible) line number (or segment + line number, if the text is divided into segments). This line number, which is a string, is stored in the key \"label\" in order to achieve consistency with [ORACC](http://oracc.org/) naming conventions. The function then calls `getline()`. The first argument of `getline()` is the part of the XML tree that belongs to a single line or gap; the second argument is `meta_d`.\n",
    "\n",
    ":::{admonition} Terminology: ORACC label vs ETCSL label\n",
    ":class: tip, dropdown\n",
    "\n",
    "In ETCSL 'label' is used for the general translation of a lemma (Guide Word in ORACC). In ORACC 'label' is reserved for human-legible references to lines, columns, obverse, reverse, etc. such as o ii 15 (obverse column 2 line 15).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsegment(tree, meta_d):\n",
    "    segments = tree.xpath('.//div1[@type=\"segment\"]')\n",
    "    \n",
    "    if segments: # if the text is not divided into segments - skip to else:\n",
    "        for snode in segments:\n",
    "            segment = snode.xpath('string(@n)')\n",
    "            for lnode in snode.xpath('.//l|.//gap'):\n",
    "                if lnode.tag == 'l':\n",
    "                    line = segment + lnode.xpath('string(@n)')\n",
    "                    meta_d[\"label\"] = line   # \"label\" is the human-legible line number\n",
    "                getline(lnode, meta_d)\n",
    "\n",
    "    else:\n",
    "        for lnode in tree.xpath('.//l|.//gap'):\n",
    "            if lnode.tag == 'l':\n",
    "                line_no = lnode.xpath('string(@n)')\n",
    "                meta_d[\"label\"] = line_no\n",
    "            getline(lnode, meta_d)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.10 Versions\n",
    "\n",
    "In some cases an [ETCSL](http://etcsl.orinst.ox.ac.uk) file contains different versions of the same composition. The versions may be distinguished as 'Version A' vs. 'Version B' or may indicate the provenance of the version ('A version from Urim' vs. 'A version from Nibru'). In the edition of the proverbs the same mechanism is used to identify tablets (often lentils) that contain just one proverb, or a few, and are collected in the files \"Proverbs from Urim,\" \"Proverbs from Nibru,\" etc. ([ETCSL](http://etcsl.orinst.ox.ac.uk) c.6.2.1 - c.6.2.5).\n",
    "\n",
    ":::{margin}\n",
    "This lentil is edited in the collection [Proverbs from Urim](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.6.2.3&display=Crit&charenc=gcirc) \"If bread is left over, the mongoose eats it. If I have bread left over, a stranger consumes it.\" (translation [ETCSL](http://etcsl.orinst.ox.ac.uk)).\n",
    "\n",
    "```{figure} ../images/P346317.jpg\n",
    ":scale: 25%\n",
    "[UET 6/2 239](http://oracc.org/epsd2/P346317): lentil from Ur with a [proverb](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.6.2.3&display=Crit&charenc=gcirc).\n",
    "```\n",
    "\n",
    "The function `getversion()` is called by the function `parsetext()` and receives two arguments: `tree` (the `etree` object) and `meta_d` (the dictionary of meta data). In the XML tree versions are marked by a node `body` with a child `head`. The node `head` contains the name of the version. Iterating through the versions, the function updates the key \"version\" in `meta_d` with the name of that version and then calls the `getsegment()` function. The first argument is the portion of the tree that represents the version that is being parsed, the second argument is `meta_d`. If a composition is not divided into versions the entire tree is passed to `getsegment()` and the version name in `meta_d` is the empty string.\n",
    "\n",
    "In some cases version names are very long and somewhat unwieldy. For instance, [The Cursing of Agade](https://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.2.1.5&display=Crit&charenc=gcirc#) has a version that is called \"Fragments of an earlier version from Nibru, dating to the Ur III period.\" This version name is abbreviated to \"Ur III\". The equivalency list `versions` (in `equivalencies/equivalencies.json`; see [2.2.2.2](2.2.2.2)) is used to adjust version names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getversion(tree, meta_d):\n",
    "    versions = tree.xpath('.//body[child::head]')\n",
    "\n",
    "    if versions: # if the text is not divided into versions - skip 'getversion()':\n",
    "        for vnode in versions:\n",
    "            version = vnode.xpath('string(head)')\n",
    "            version = eq[\"versions\"].get(version, version)\n",
    "            meta_d[\"version\"] = version\n",
    "            getsegment(vnode, meta_d)\n",
    "\n",
    "    else:\n",
    "        meta_d[\"version\"] = ''\n",
    "        getsegment(tree, meta_d)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2.2.11)=\n",
    "## 2.2.11 Parse a Text\n",
    "\n",
    "The main process iterates through the list of XML files (one for each composition in [ETCSL](https://etcsl.orinst.ox.ac.uk/)) and calls for each of them the function `parsetext()`. After opening the XML file `parsetext()` first calls the function `ampersands()` in order to replace HTML entities by their unicode counterparts ([2.2.3](2.2.3)). The module `etree` from the `lxml` library is used to read the XML tree. Since `etree` does not read the XML directly from file, but rather reads the output of the `ampersands()` function, we need the function `fromstring()`:\n",
    "\n",
    "After creating the tree, the function `mark_extra()` ([2.2.4](2.2.4)) is called in order to explicitly mark \"additional\" and \"secondary\" words.  The composition name is found in the node `title`. This name is slightly adjusted in two ways. First, all [ETCSL](https://etcsl.orinst.ox.ac.uk/) text names include the phrase \" -- a composite transliteration\". This is useful for online presentation, but not for computational text analysis. Second, some titles include commas, which may create problems when data are saved in `cvs` format. These two elements are removed from the title.\n",
    "\n",
    "The dictionary `meta_d`, which was created as an empty dictionary in the main process, is now filled with meta data on the composition level: the text ID (the [ETCSL](https://etcsl.orinst.ox.ac.uk/) text number, for instance c.1.4.1 for Inana's Descent) and the text name. Finally, the line reference count is set to 0. This line reference is updated every time the function `getline()` is called.\n",
    "\n",
    "The XML tree is now forwarded to the function `getversion()` together with the dictionary `meta_d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsetext(file, meta_d):\n",
    "    with open(f'etcsl/transliterations/{file}') as f:\n",
    "        xmltext = f.read()\n",
    "    xmltext = ampersands(xmltext)          #replace HTML entities by Unicode equivalents\n",
    "    \n",
    "    tree = etree.fromstring(xmltext)\n",
    "    \n",
    "    tree = mark_extra(tree, \"additional\") # mark additional words with attribute status = 'additional'\n",
    "    tree = mark_extra(tree, \"secondary\")  # mark secondary words with attribute status = 'secondary'\n",
    "    name = tree.xpath('string(//title)')\n",
    "    name = name.replace(' -- a composite transliteration', '')\n",
    "    name = name.replace(',', '')\n",
    "    meta_d[\"id_text\"] =  file[:-4]\n",
    "    meta_d[\"text_name\"] = name\n",
    "    meta_d[\"id_line\"] = 0\n",
    "    getversion(tree, meta_d)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2.2.12)=\n",
    "## 2.2.12 Main Process\n",
    "\n",
    "The list `alltexts` is created as an empty list. It will be filled with dictionaries, each dictionary representing one word form.\n",
    "\n",
    "The variable `textlist` is a list of all the XML files with [ETCSL](http://etcsl.orinst.ox.ac.uk) compositions in the directory `etcsl/transliterations`. Iterating through this list, each file  is sent as an argument to the function `parsetext()`. \n",
    "\n",
    "The dictionary `meta_d` is created as an empty dictionary. On each level of analysis the dictionary is updated with meta-data, such as text ID, version name, line number, etc.\n",
    "\n",
    "The main process also defines a number of variables (compiled regular expressions and translation tables) that are used later on in the process for adjusting transliteration conventions in the function `tounicode()` (see [2.2.5](2.2.5))\n",
    "\n",
    "After the loop has gone through all the file names (this may take a few minutes) the list `alltexts` is transformed into a `pandas` DataFrame. The progress bar should indicate that 394 files have been processed and the resulting dataframe should have 12 columns and 170,856 rows, each row representing a single word or a gap. All missing values (`NaN`) are replaced by empty strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebe062e542e4c20b8e08419b61bb8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "textlist = os.listdir('etcsl/transliterations')\n",
    "textlist = [file for file in textlist if file[-4:] == '.xml']\n",
    "textlist.sort()\n",
    "\n",
    "amp = re.compile(r'&[^;]+;') #regex for HTML entities, used in ampersands()\n",
    "\n",
    "asccj, unicj = 'cjCJ', 'šŋŠŊ'\n",
    "transcj = str.maketrans(asccj, unicj) # translation table for c > š and j > ŋ\n",
    "\n",
    "ind = re.compile(r'[a-zŋḫṣšṭA-ZŊḪṢŠṬ][0-9x]{1,2}') #regex for sign index nos preceded by a letter\n",
    "ascind, uniind = '0123456789x', '₀₁₂₃₄₅₆₇₈₉ₓ'\n",
    "transind = str.maketrans(ascind, uniind) # translation table for index numbers\n",
    "# regex ind and the translation tables transind and transcj are used in tounicode()\n",
    "\n",
    "alltexts = []\n",
    "files = tqdm(textlist)\n",
    "for file in files:\n",
    "    files.set_description(f'ETCSL {file[2:-4]}')\n",
    "    meta_d = {}\n",
    "    parsetext(file, meta_d)\n",
    "\n",
    "df = pd.DataFrame(alltexts).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>text_name</th>\n",
       "      <th>id_line</th>\n",
       "      <th>version</th>\n",
       "      <th>label</th>\n",
       "      <th>cf</th>\n",
       "      <th>gw</th>\n",
       "      <th>pos</th>\n",
       "      <th>form</th>\n",
       "      <th>lang</th>\n",
       "      <th>extent</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c.0.1.1</td>\n",
       "      <td>Ur III catalogue from Nibru (N1)</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>dubsaŋ</td>\n",
       "      <td>first</td>\n",
       "      <td>AJ</td>\n",
       "      <td>dub-saŋ-ta</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c.0.1.1</td>\n",
       "      <td>Ur III catalogue from Nibru (N1)</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>Enki</td>\n",
       "      <td>1</td>\n",
       "      <td>DN</td>\n",
       "      <td>{d}en-ki</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c.0.1.1</td>\n",
       "      <td>Ur III catalogue from Nibru (N1)</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>unu</td>\n",
       "      <td>dwelling</td>\n",
       "      <td>N</td>\n",
       "      <td>unu₂</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c.0.1.1</td>\n",
       "      <td>Ur III catalogue from Nibru (N1)</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>gal</td>\n",
       "      <td>big</td>\n",
       "      <td>V/i</td>\n",
       "      <td>gal</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c.0.1.1</td>\n",
       "      <td>Ur III catalogue from Nibru (N1)</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>ed</td>\n",
       "      <td>ascend</td>\n",
       "      <td>V/i</td>\n",
       "      <td>im-ed₃</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170851</th>\n",
       "      <td>c.6.2.5</td>\n",
       "      <td>Proverbs: of unknown provenance</td>\n",
       "      <td>211</td>\n",
       "      <td>YBC 9912</td>\n",
       "      <td>1</td>\n",
       "      <td>haš</td>\n",
       "      <td>thigh</td>\n",
       "      <td>N</td>\n",
       "      <td>haš₂</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170852</th>\n",
       "      <td>c.6.2.5</td>\n",
       "      <td>Proverbs: of unknown provenance</td>\n",
       "      <td>211</td>\n",
       "      <td>YBC 9912</td>\n",
       "      <td>1</td>\n",
       "      <td>gid</td>\n",
       "      <td>long</td>\n",
       "      <td>V/i</td>\n",
       "      <td>ba-ra-an-gid₂-nam</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170853</th>\n",
       "      <td>c.6.2.5</td>\n",
       "      <td>Proverbs: of unknown provenance</td>\n",
       "      <td>211</td>\n",
       "      <td>YBC 9912</td>\n",
       "      <td>1</td>\n",
       "      <td>lu</td>\n",
       "      <td>person</td>\n",
       "      <td>N</td>\n",
       "      <td>lu₂</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170854</th>\n",
       "      <td>c.6.2.5</td>\n",
       "      <td>Proverbs: of unknown provenance</td>\n",
       "      <td>211</td>\n",
       "      <td>YBC 9912</td>\n",
       "      <td>1</td>\n",
       "      <td>ŋeši</td>\n",
       "      <td>sesame</td>\n",
       "      <td>N</td>\n",
       "      <td>še-ŋiš-i₃</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170855</th>\n",
       "      <td>c.6.2.5</td>\n",
       "      <td>Proverbs: of unknown provenance</td>\n",
       "      <td>211</td>\n",
       "      <td>YBC 9912</td>\n",
       "      <td>1</td>\n",
       "      <td>il</td>\n",
       "      <td>raise</td>\n",
       "      <td>V/t</td>\n",
       "      <td>bi₂-ib₂-il₂-il₂</td>\n",
       "      <td>sux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170856 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_text                         text_name  id_line   version label  \\\n",
       "0       c.0.1.1  Ur III catalogue from Nibru (N1)        1               1   \n",
       "1       c.0.1.1  Ur III catalogue from Nibru (N1)        2               2   \n",
       "2       c.0.1.1  Ur III catalogue from Nibru (N1)        2               2   \n",
       "3       c.0.1.1  Ur III catalogue from Nibru (N1)        2               2   \n",
       "4       c.0.1.1  Ur III catalogue from Nibru (N1)        2               2   \n",
       "...         ...                               ...      ...       ...   ...   \n",
       "170851  c.6.2.5   Proverbs: of unknown provenance      211  YBC 9912     1   \n",
       "170852  c.6.2.5   Proverbs: of unknown provenance      211  YBC 9912     1   \n",
       "170853  c.6.2.5   Proverbs: of unknown provenance      211  YBC 9912     1   \n",
       "170854  c.6.2.5   Proverbs: of unknown provenance      211  YBC 9912     1   \n",
       "170855  c.6.2.5   Proverbs: of unknown provenance      211  YBC 9912     1   \n",
       "\n",
       "            cf        gw  pos               form lang extent status  \n",
       "0       dubsaŋ     first   AJ         dub-saŋ-ta  sux                \n",
       "1         Enki         1   DN           {d}en-ki  sux                \n",
       "2          unu  dwelling    N               unu₂  sux                \n",
       "3          gal       big  V/i                gal  sux                \n",
       "4           ed    ascend  V/i             im-ed₃  sux                \n",
       "...        ...       ...  ...                ...  ...    ...    ...  \n",
       "170851     haš     thigh    N               haš₂  sux                \n",
       "170852     gid      long  V/i  ba-ra-an-gid₂-nam  sux                \n",
       "170853      lu    person    N                lu₂  sux                \n",
       "170854    ŋeši    sesame    N          še-ŋiš-i₃  sux                \n",
       "170855      il     raise  V/t    bi₂-ib₂-il₂-il₂  sux                \n",
       "\n",
       "[170856 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.13 Save as CSV\n",
    "\n",
    "The DataFrame that is the result of the notebook is saved as a `csv` file named `alltexts.csv`, where each word form occupies a single row. In many cases, however, we may want to represent the data in a line-by-line or composition-by-composition format and/or filter out certain words (for instance: use only lemmatized words, remove Akkadian words, remove \"additional\" and/or \"secondary\" text - etc.). Such transformations can be done most easily in a `pandas` dataframe. \n",
    "\n",
    ":::{margin}\n",
    "\n",
    "For manipulating a dataframe in `pandas`, see Chapter 2.1: Data Acquisition ORACC (for instance the sections [2.1.2.6](2.1.2.6) and [2.1.3.8](2.1.3.8)).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/alltexts.csv', 'w', encoding=\"utf-8\") as w:\n",
    "    df.to_csv(w, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}